{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7dbb77-ab43-4517-98b7-6c0084bc128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def downsize_wav(input_path, output_path, target_bitrate):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "\n",
    "    # Set the target bitrate\n",
    "    audio = audio.set_frame_rate(target_bitrate)\n",
    "\n",
    "    # Export the downsized audio to the output file\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "\n",
    "# Example usage\n",
    "input_wav_path = \"output.wav\"\n",
    "output_wav_path = \"output.wav\"\n",
    "target_bitrate = 16000  # Set your desired bitrate\n",
    "\n",
    "downsize_wav(input_wav_path, output_wav_path, target_bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b563f1-0695-40da-9df7-1de567dff86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: chunk_1.wav\n",
      "Response: {\"result\":0,\"return_type\":\"com.google.gson.internal.LinkedTreeMap\",\"return_object\":{\"recognized\":\"안녕하세요, 페스트 포크스 포를 스타 최요은입니다.\"}}\n",
      "b'{\"result\":0,\"return_type\":\"com.google.gson.internal.LinkedTreeMap\",\"return_object\":{\"recognized\":\"\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94, \\xed\\x8e\\x98\\xec\\x8a\\xa4\\xed\\x8a\\xb8 \\xed\\x8f\\xac\\xed\\x81\\xac\\xec\\x8a\\xa4 \\xed\\x8f\\xac\\xeb\\xa5\\xbc \\xec\\x8a\\xa4\\xed\\x83\\x80 \\xec\\xb5\\x9c\\xec\\x9a\\x94\\xec\\x9d\\x80\\xec\\x9e\\x85\\xeb\\x8b\\x88\\xeb\\x8b\\xa4.\"}}'\n",
      "File chunk_1.wav deleted.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     51\u001b[0m     audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m     \u001b[43msplit_and_transcribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36msplit_and_transcribe\u001b[0;34m(audio_file_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m     18\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 35\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m request_json \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_code\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkorean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio_content\n\u001b[1;32m     31\u001b[0m     }\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m http \u001b[38;5;241m=\u001b[39m urllib3\u001b[38;5;241m.\u001b[39mPoolManager()\n\u001b[0;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json; charset=UTF-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAuthorization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# API 응답 처리\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/urllib3/_request_methods.py:118\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    111\u001b[0m         method,\n\u001b[1;32m    112\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/urllib3/_request_methods.py:217\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    213\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    215\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/urllib3/poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pydub import AudioSegment\n",
    "import urllib3\n",
    "import json\n",
    "import base64\n",
    "\n",
    "api_key =\n",
    "api_url =\n",
    "\n",
    "def split_and_transcribe(audio_file_path):\n",
    "    # 음성 파일을 5초 간격으로 분할\n",
    "    audio = AudioSegment.from_file(audio_file_path, format=\"wav\")\n",
    "    interval = 10 * 1000  # milliseconds\n",
    "    chunks = [audio[i:i + interval] for i in range(0, len(audio), interval)]\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.export(f\"chunk_{i+1}.wav\", format=\"wav\")\n",
    "        transcribe_audio(f\"chunk_{i+1}.wav\")\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    # ETRI 음성 인식 API 호출\n",
    "    with open(file_path, 'rb') as audio_file:\n",
    "        audio_content = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "    audio_file.close()\n",
    "\n",
    "    request_json = {\n",
    "        \"argument\": {\n",
    "            \"language_code\": \"korean\",\n",
    "            \"audio\": audio_content\n",
    "        }\n",
    "    }\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        api_url,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\": api_key},\n",
    "        body=json.dumps(request_json)\n",
    "    )\n",
    "\n",
    "    # API 응답 처리\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(\"Response: \"+str(response.data,\"utf-8\"))\n",
    "    print(response.data)\n",
    "\n",
    "    os.remove(file_path)\n",
    "    print(f\"File {file_path} deleted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = \"./output.wav\"\n",
    "    split_and_transcribe(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c9a2935-528d-4dd7-9f72-da305ee0e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'rcw0sfVmRyGoox2PErxEpA'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "config = {}\n",
    "resp = requests.post(\n",
    "    'https://openapi.vito.ai/v1/transcribe',\n",
    "    headers={'Authorization': 'bearer '+resp.json()['access_token']},\n",
    "    data={'config': json.dumps(config)},\n",
    "    files={'file': open('output.wav', 'rb')}\n",
    ")\n",
    "resp.raise_for_status()\n",
    "print(resp.json())\n",
    "transcribe_id = resp.json()['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac9db746-f5f4-4779-bc77-8b43caf1135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 파스텔 커피, 워스, 바리스타 정혜원입니다.\n",
      "이번 영상에서는 파스텔 커피웍스가 투가식으로 따뜻한 브리잉을 어떻게 내리는지 소개해 드리려고 합니다.\n",
      "저희 매장에서는 하리오 스위치 드리퍼를 사용해서 보통 침지식으로 커피를 추출을 하고 있지만, 처음부터 스위치를 내려서 사용하시게 되면은 투가식으로도 커피를 추출해 내실 수 있어요.\n",
      "보통 사용하시는 드리퍼가 하리오의 브이식 스트시거나 아니면 나눠서 투가식으로 커피를 추출하시는 분들에게는 이 방법을 권장해 드립니다.\n",
      "오늘 함께 추출해 볼 커피는 저희 언스페셜티 블랜드 프루티 버전 투입니다. 추출해 사용할 원도의 양은 총 19g입니다.\n",
      "군세도는 2, K 43, S 기준으로 11입니다.\n",
      "문쇄한 커피를 담기 전에는 꼭 드리퍼와 서버를 결합을 해주시고, 종이 필터를 넣은 채 린싱을 진행해 주세요.\n",
      "종이 필터 린싱을 해주시게 되시면 종이 필터 냄새를 제거할 수도 있고.\n",
      "또 저희가 목적으로 하고 있는 이 92도의 온도가 내려가지 않도록 방지하는 효과를 줄 수 있습니다.\n",
      "이 커피 성분이 필터에 협착되는 것을 방지할 수 있어요.\n",
      "충분히 린싱이 되었으면 물을 버리고 다시 드리퍼와 서버를 결합을 해주시고요.\n",
      "이번에는 추가식으로 진행할 거기 때문에 스위치가 내려져 있는지 꼭 확인을 해주세요.\n",
      "그리 분쇄한 커피 18g을 담도록 하겠습니다. 커피 표면을 드리퍼를 살짝 흔들어서 평평하게 해주시고요.\n",
      "이제 너무 드리퍼를 세게 흔드는 것보다는 가볍게 2, 3회 정도 흔들어서 평평하게 만들어 주시는 것을 추천해 드립니다.\n",
      "저울 영 점을 잡으시고요. 92 도의 추출수를 준비를 해주세요.\n",
      "뜸은 30g의 물을 가지고 30초 동안 드리도록 하겠습니다.\n",
      "그리고 1차 포어는 30초가 지나면 100g의 물을 추가적으로 보어주도록 하겠습니다.\n",
      "30초가 지나고 1분이 되었을 때 다시 100g의 불을 추가적으로 부어지도록 하겠습니다.\n",
      "1분 30초가 되면은 마지막으로 50g을 부어줄 텐데요. 이때는 가운데에서 물을 부어주시면 되겠습니다.\n",
      "그래서 280 이 될 때까지 가운데에서 센터 포어로 진행해 주시면 됩니다.\n",
      "추출은 2분 30초에서 3분 내로 종료가 됩니다.\n",
      "만약에 3분을 넘어 훌쩍 넘는 시간이 걸렸다면은 분세들을 조금 굵게 가져가시는 것을 추천드립니다.\n",
      "이제 드리퍼를 서버에서 제거하고 두 번 저어주세요. 그리고 미리 예열한 잔 내에 담아서 드시면 됩니다.\n",
      "오늘 추출한 푸르티 버전 2 를 마셔보도록 하겠습니다. 프루티 버전 2 에서는요.\n",
      "감귤의 선리가 잘 느껴지고 복숭아의 뉘앙스도 잘 느끼실 수 있어요.\n",
      "그리고 부드러운 바닐라의 향리와 함께 위크 초콜릿의 단맛이 잘 받춰주고 있습니다.\n",
      "오늘 영상 시청해 주셔서 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def check_transcribe_status(transcribe_id, access_token):\n",
    "    while True:\n",
    "        resp = requests.get(\n",
    "            'https://openapi.vito.ai/v1/transcribe/'+transcribe_id,\n",
    "            headers={'Authorization': 'bearer '+ access_token},\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        response_json = resp.json()\n",
    "\n",
    "        status = response_json.get('status', '')\n",
    "\n",
    "        if status == 'completed':\n",
    "            for i in resp.json()['results']['utterances']:\n",
    "                print(i['msg'])\n",
    "            break\n",
    "        elif status == 'transcribing':\n",
    "            print('Transcribing...')\n",
    "        else:\n",
    "            print(f'Unexpected status: {status}')\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "check_transcribe_status(transcribe_id, access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ba854cb-a4c9-40b5-8605-73ebcaebeb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def split_mp3(input_file, output_prefix, interval=5000):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(input_file, \"mp3\")\n",
    "    except:\n",
    "        audio = AudioSegment.from_file(input_file, format=\"mp4\")\n",
    "\n",
    "    # Calculate the number of segments\n",
    "    num_segments = len(audio) // interval + 1\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start_time = i * interval\n",
    "        end_time = (i + 1) * interval\n",
    "        segment = audio[start_time:end_time]\n",
    "\n",
    "        output_file = f\"{output_prefix}_segment_{i + 1}.mp3\"\n",
    "        segment.export(output_file, format=\"mp3\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_mp3 = \"핸드드립이 불티나게 팔리는 카페의 추출 레시피를 공개합니다 (핫아이스).mp3\"\n",
    "    output_prefix = \"output_segment\"\n",
    "\n",
    "    split_mp3(input_mp3, output_prefix, interval=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eff17416-ce9c-4c4e-b3ac-2083bbf9e6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube url https://www.youtube.com/watch?v=QrFYMtP0cRo&list=PLUAYxJFHd-c72PAaPx416sJ9jUlBKWojt&index=2\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "import os\n",
    "\n",
    "link = input(\"youtube url\")\n",
    "\n",
    "yt = YouTube(link)\n",
    "mp4_file_path = yt.streams.filter(only_audio=True).first().download()\n",
    "file_path = mp4_file_path.replace('mp4','mp3')\n",
    "os.rename(mp4_file_path, file_path)\n",
    "os.rename(file_path, \"your_input_file.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f17e160-73a1-4a70-b7f8-5f192ba8020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def cut_mp3(input_file, output_files, durations):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(input_file, \"mp3\")\n",
    "    except:\n",
    "        audio = AudioSegment.from_file(input_file, format=\"mp4\")\n",
    "\n",
    "    if len(output_files) != len(durations):\n",
    "        raise ValueError(\"Number of output files should match the number of durations.\")\n",
    "\n",
    "    start = times[0] *1000\n",
    "    for i, duration in enumerate(durations):\n",
    "        end = start + duration * 1000  # Convert duration to milliseconds\n",
    "        segment = audio[start:end+2]\n",
    "        segment.export(output_files[i], format=\"mp3\")\n",
    "        start = end\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"your_input_file.mp3\"  # Replace with your input file\n",
    "    output_files = [\"output_1.mp3\", \"output_2.mp3\"]  # Replace with desired output file names\n",
    "\n",
    "    global times\n",
    "    times= [0, 296, 501]\n",
    "    durations = []\n",
    "    for i in range(len(times)-1):\n",
    "        durations.append(times[i+1]-times[i])\n",
    "\n",
    "    cut_mp3(input_file, output_files, durations)\n",
    "\n",
    "    os.remove(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4627db17-d1f3-4a17-bd94-bfe87827cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing...\n",
      "Transcribing...\n",
      "Transcribing...\n",
      "Transcribing...\n",
      "result:\n",
      "[['안녕하세요, 디플루레이크 이철원입니다. 아이스는 핫이랑 분쇄도는 똑같아요. 그 대신 커피 양이 달라지고요.', '예, 투출양도 좀 달라집니다.', '먼저 서버에 얼음 100g을 담아주세요, 마찬가지로 린싱은 충분히 적셔질 때까지 잘 해주시면 좋고요.', '커피는 22g 사용합니다. 그리고 내리는 양은 221대 10 비율이에요. 간단하죠. 오늘 갈를게요.', '아이스와 사컵은 똑같은 분세도를 사용하고 있어요. 지금 이 원스펠티 베리베리, 웨이크 블랜딩 향이 진짜 엄청나네요.', '맛있겠는데 그래서 좀 다른 거는 어떻게 하냐면 처음에 많이 담았기 때문에 뜸들이는 양도 많아지고요.', '그래서 60g의 뜸을 들여야 되고, 그다음에 첫 번째 내렸을 때가 110g을 내려요.', '앞부분을 더 많이 뽑아낼 수 있게끔 만들어지고, 그다음 170g까지 맞춘 다음에 220g까지 총 50g 내리시면 됩니다.', '처음이 뜸 60g, 110g, 50g, 물론 도는 똑같이 93도를 쓸게요.', '쓸 마찬가지로 처음이랑 두 번째는 와류를 줍니다. 뜸은 60g을 쓸 거고요.', '충분히 접세지 있게 가늘게 꿈을 들여주시면 됩니다. 커피 양이 많아지기 때문에 뜸 들리는 물 양도 당연히 많아져야 돼요.', '그리고 똑같이 30초가 지났을 때 지금 내리는 거는 언스펠티에 블랜드고요, 레이크 베리베리, 레이크, 블랜딩이에요.', '와류를 똑같은 방향으로 한 방을 주시고 그다음에 천천히 부어주시면 됩니다.', '이 블루밍이 깨지지 않게끔 저는 유지를 많이 하려고 하는 편이에요.', '어떤 경우는 보면은 이렇게 안에를 파서 일정 야게끔 만드시잖아요.', '이게 어떻게 보면 처음 내리고 나서 보면은 똑같은 그런 모양이 돼요.', '1분이 됐을 때 마지막으로 한 번 더 똑같은 방향으로 천천히 230g 지금 내렸습니다.', '220 에서 230 정도 내리셔도 상관은 없어요. 간단하죠, 끝이에요.', '다 내려갈 때까지 내린 다음 물이 다 내려오면 그다음에 얼음 100g을 다시 추가로 넣어줍니다.', '내리는 시간은 보통 2분 정도까지 보시면 돼요. 엄청 짧아요.', '그리고 어느 정도 다 내렸다고 생각하시면 그냥 간단하게 기다리시지 말고 그냥 빼셔도 됩니다.', '얼음 100g을 맞춰서 얼음 100g을 하셨다면 얼음과 이 커피 음료가 제대로 안 섞여 있기 때문에 여러 번 돌려서.'], ['빠시면 되고요. 한번 셔서 다 이렇게 드시면 돼요, 저도 한번 맛을 볼게요.', '제 컵을 따로 해서 베리베리 블린딩은 좀 어떻게 보면은 올 주가를 좋아하시는 월피커 분들을 위해서 만든 커피인 것 같기도 해요.', '왜냐하면 워시드 커피보다 네 트러 커피가 아이스커피에 더 잘 어울린다고 생각하거든요.', '아이스커피는 이렇게 즐겨주시면 너무 감사할 것 같아요.']]\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "import sys\n",
    "import os\n",
    "import text_extraction\n",
    "from pydub import AudioSegment\n",
    "import fnmatch\n",
    "\n",
    "def find_files(directory, pattern):\n",
    "    files = []\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(os.path.join(root, filename))\n",
    "\n",
    "    return files\n",
    "\n",
    "def get_yt_video(link):\n",
    "    yt = YouTube(link)\n",
    "    mp4_file_path = yt.streams.filter(only_audio=True).first().download()\n",
    "    file_path = mp4_file_path.replace('mp4','mp3')\n",
    "    os.rename(mp4_file_path, file_path)\n",
    "    return file_path\n",
    "\n",
    "def get_yt_text(link):\n",
    "    text_list = []\n",
    "\n",
    "    if os.path.exists(link):\n",
    "        file_path = link\n",
    "    else:\n",
    "        file_path = get_yt_video(link)\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path, \"mp3\")\n",
    "    except:\n",
    "        audio = AudioSegment.from_file(file_path, format=\"mp4\")\n",
    "\n",
    "    interval = 180 * 1000\n",
    "    chunks = [audio[i:i + interval] for i in range(0, len(audio), interval)]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.export(f\"chunk_{i+1}.mp3\", format=\"mp3\")\n",
    "        text_list.append(text_extraction.get_text(f\"chunk_{i+1}.mp3\"))\n",
    "\n",
    "    os.remove(file_path)\n",
    "    return text_list\n",
    "\n",
    "def save_text_to_file(text_list, base_filename=\"test\"):\n",
    "    index = 1\n",
    "    filename = f\"{base_filename}_{index}.txt\"\n",
    "\n",
    "    while os.path.exists(filename):\n",
    "        index += 1\n",
    "        filename = f\"{base_filename}_{index}.txt\"\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        for sublist in text_list:\n",
    "            file.write(\"\\n\".join(sublist) + \"\\n\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    directory = \"/home/woong/coffee_recipe_generator/\"\n",
    "    pattern = \"output_*.mp3\"\n",
    "\n",
    "    matching_files = find_files(directory, pattern)\n",
    "\n",
    "    if matching_files:\n",
    "        for file in matching_files:\n",
    "            text = get_yt_text(file)\n",
    "            result_filename = save_text_to_file(text)\n",
    "    else:\n",
    "        if len(sys.argv) !=2:\n",
    "            print(\"Usage: python main.py <youtube_link or file_path>\")\n",
    "            sys.exit(1)\n",
    "        youtube_link = sys.argv[1]\n",
    "        text = get_yt_text(youtube_link)\n",
    "        result_filename = save_text_to_file(text)\n",
    "\n",
    "    print(\"result:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c4a9db-81c4-47d2-8eda-77872d63bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Sentence  Label\n",
      "0                         안녕하세요, 반갑습니다. 폰트 커피 양태은입니다.\\n      0\n",
      "1                이번에 제가 알려드릴 레시피는 아이스 브루윙, 커피 레시피입니다.\\n      0\n",
      "2     저희 브루윙 레시피의 중요한 점은 상대적으로 굵은 물줄기를 사용하시는 것과 추출 시...      1\n",
      "3     조금 더 쉽게 말씀드리면 저희 커피는 대개 추출이 3분 전으로 종료되는데, 이때 푸...      1\n",
      "4                         그럼 커피를 내리면서 자세히 설명해 드리겠습니다.\\n      0\n",
      "...                                                 ...    ...\n",
      "1095  그래서 생각했을 때 산미나 향 톤이 좀 더 강하게 나오지 않을까? 그리고 위에 있는...      0\n",
      "1096  대략적으로 3분을 채웠어요. 보통 브르잉은 3분에서 3분 30초 정도 내리시면 됩니...      0\n",
      "1097  드실 때 방법은 저 같은 경우는 향을 먼저 중시하기 때문에 흔들어서 흔코리 맡아서 ...      0\n",
      "1098          그냥 한 번에 드시는 것보다는 처음에 입안에 먹금 고서 골르시면 돼요.\\n      0\n",
      "1099  커피 한 잔 드시면서 지친 일상에서 잠깐 여유를 가지시고 즐기시기 바라면서 오늘 하...      0\n",
      "\n",
      "[1100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 파일이 들어있는 디렉토리 설정\n",
    "directory_path = '/home/woong/coffee_recipe_generator/'  # 여러 파일이 들어있는 디렉토리 경로로 대체\n",
    "\n",
    "# 파일 목록 가져오기\n",
    "file_list = [f for f in os.listdir(directory_path) if f.startswith(\"test_\") and f.endswith(\".txt\")]\n",
    "\n",
    "# 데이터 프레임 초기화\n",
    "data = {'Sentence': [], 'Label': []}\n",
    "\n",
    "# 파일별로 읽어오기\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        labels = [1 if (\"추출\" or \"푸어\" or \"부어\") in line else 0 for line in lines]\n",
    "\n",
    "        # 데이터 추가\n",
    "        data['Sentence'].extend(lines)\n",
    "        data['Label'].extend(labels)\n",
    "\n",
    "# DataFrame 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame 확인\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208eede3-fa23-4499-a98f-f897265bbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'output.csv'\n",
    "df.to_csv(path, index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae3d48-9b05-41ba-acbb-920294b916ee",
   "metadata": {},
   "source": [
    "<h3>output.csv </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c8a26c-c2b6-42c4-9443-9a9fdc56ea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Sentence  Label\n",
      "0                         안녕하세요, 반갑습니다. 폰트 커피 양태은입니다.\\n      0\n",
      "1                이번에 제가 알려드릴 레시피는 아이스 브루윙, 커피 레시피입니다.\\n      0\n",
      "2     저희 브루윙 레시피의 중요한 점은 상대적으로 굵은 물줄기를 사용하시는 것과 추출 시...      0\n",
      "3     조금 더 쉽게 말씀드리면 저희 커피는 대개 추출이 3분 전으로 종료되는데, 이때 푸...      0\n",
      "4                         그럼 커피를 내리면서 자세히 설명해 드리겠습니다.\\n      0\n",
      "...                                                 ...    ...\n",
      "1095  그래서 생각했을 때 산미나 향 톤이 좀 더 강하게 나오지 않을까? 그리고 위에 있는...      0\n",
      "1096  대략적으로 3분을 채웠어요. 보통 브르잉은 3분에서 3분 30초 정도 내리시면 됩니...      1\n",
      "1097  드실 때 방법은 저 같은 경우는 향을 먼저 중시하기 때문에 흔들어서 흔코리 맡아서 ...      0\n",
      "1098          그냥 한 번에 드시는 것보다는 처음에 입안에 먹금 고서 골르시면 돼요.\\n      0\n",
      "1099  커피 한 잔 드시면서 지친 일상에서 잠깐 여유를 가지시고 즐기시기 바라면서 오늘 하...      0\n",
      "\n",
      "[1100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d2fdcd-9582-4f6f-8888-50bb7d1de407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "x_data = df['Sentence']\n",
    "y_data = df['Label']\n",
    "\n",
    "print(len(x_data))\n",
    "print(len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62759d10-c88f-4d55-b09b-af883b2a8bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 02:55:04.440799: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-13 02:55:04.440866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-13 02:55:04.444533: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-13 02:55:04.464514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 02:55:05.202293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7769c529-073c-416f-a325-a5de52b6d8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 300\n"
     ]
    }
   ],
   "source": [
    "num_train_data = 800\n",
    "num_test_data = len(sequences) - num_train_data\n",
    "\n",
    "print(num_train_data, num_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b241c7b8-62aa-4442-91bf-29b33f09acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "12.446363636363637\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( max(len(i) for i in sequences))\n",
    "print( sum(map(len, sequences))/len(sequences))\n",
    "plt.hist([len(s) for s in sequences], bins = 50)\n",
    "plt.xlabel('length of Data')\n",
    "plt.ylabel('number of Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e84edb84-2c9b-463a-9d97-258be5e59c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequences[:num_train_data]\n",
    "x_test = sequences[:num_train_data]\n",
    "\n",
    "y_train = y_data[:num_train_data]\n",
    "y_test = y_data[:num_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd509454-effb-4f8c-93aa-435887565848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 982, 285, 244, 79, 983, 984, 200, 985, 14, 80, 986, 299, 987, 988, 39, 11]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[10])\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b591b1d-23d7-4e12-b668-7e1b07c5a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(770,)\n",
      "(165,)\n",
      "(165,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Find the maximum length of sequences\n",
    "max_len = max(max(len(l) for l in x_train), max(len(l) for l in x_validation), max(len(l) for l in x_test))\n",
    "\n",
    "# # Pad sequences\n",
    "# x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "# x_validation = pad_sequences(x_validation, maxlen=max_len)\n",
    "# x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6470dd-cd40-4434-8080-af53065a2f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221    저희는 네추럴 커피 같은 경우에는 93도 정도로 추천을 드리고, 물 온도를 헌이나 ...\n",
      "235           그리고 원두 같은 경우에는 저희 같은 경우에는 싱글로리징 같은 경우에는.\\n\n",
      "433    그리고 산미, 그리고 복합성이 같이 잘 나와서 굉장히 청장하고 맛있는 아이스 브로잉...\n",
      "599     필터는 리브와 충분히 접촉될 수 있도록 조금 여유를 두고 접어주시는 편이 좋습니다.\\n\n",
      "305    또 저희 커피를 내려드실 수 있는 레시피를 공유해 드리려고 합니다. 이번 영상에는 ...\n",
      "Name: Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be9e80c-03e1-4de0-baaa-2d9e8efd7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train.tolist())\n",
    "sequences = tokenizer.texts_to_sequences(x_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18618f20-c9d2-4e14-9c58-4a9ce6e51b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msequence\u001b[49m[:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence' is not defined"
     ]
    }
   ],
   "source": [
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7317a642-c578-44c4-b674-375f1b0aa316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2553\n",
      "{'수': 1, '같은': 2, '그리고': 3, '더': 4, '이': 5, '커피를': 6, '하겠습니다': 7, '됩니다': 8, '추출': 9, '물': 10, '때': 11, '한': 12, '물을': 13, '경우에는': 14, '잘': 15, '커피': 16, '있습니다': 17, '그래서': 18, '충분히': 19, '저희가': 20, '저희': 21, '드리퍼를': 22, '좀': 23, '총': 24, '추출이': 25, '조금': 26, '이번': 27, '지금': 28, '아이스': 29, '추출을': 30, '정도': 31, '다': 32, '하리오': 33, '때문에': 34, '주시면': 35, '커피는': 36, '제가': 37, '때는': 38, '있도록': 39, '있는': 40, '30초': 41, '1분': 42, '2분': 43, '번': 44, '안녕하세요': 45, '물이': 46, '린싱을': 47, '정도로': 48, '합니다': 49, '꼭': 50, '레시피를': 51, '온도는': 52, '사용하고': 53, '종이': 54, '너무': 55, '오늘': 56, '할': 57, '굉장히': 58, '감사합니다': 59, '되면': 60, '43': 61, '사용할': 62, '또': 63, '한번': 64, '이제': 65, '다시': 66, '것을': 67, '있어요': 68, '필터를': 69, '보다': 70, '브루잉': 71, '되겠습니다': 72, '있는지': 73, '많이': 74, '기준으로': 75, '1대': 76, '해주세요': 77, '게': 78, '그다음에': 79, '필터': 80, '소개해': 81, '30초가': 82, '서버에': 83, '추출한': 84, '것': 85, '60g': 86, '뜸은': 87, '에서': 88, '평평하게': 89, '시간은': 90, 'v': 91, '가지고': 92, '사용해서': 93, '돼요': 94, '있고': 95, '그냥': 96, '흔들어서': 97, '분쇄도는': 98, '다음에': 99, '천천히': 100, '레시피는': 101, '영상에서는': 102, '원두': 103, '동안': 104, '얼음': 105, '가볍게': 106, '후': 107, '비율로': 108, '추출할': 109, '하는': 110, '스위치를': 111, '커피의': 112, '때까지': 113, '줄': 114, '될': 115, '만약에': 116, '두': 117, '번째': 118, '하고': 119, '진행해': 120, '거는': 121, '뜸을': 122, 'ek': 123, '따뜻한': 124, '먼저': 125, '내릴': 126, '2': 127, '이렇게': 128, '그럼': 129, '후에': 130, '물로': 131, '92도의': 132, '되었을': 133, '커피가': 134, '어떻게': 135, '주시고': 136, '마이크론': 137, '10': 138, '드리퍼의': 139, '그': 140, '3분': 141, '보통': 142, '세': 143, '분쇄한': 144, '함께': 145, '침지식으로': 146, '그렇게': 147, '건데요': 148, '저희는': 149, '6': 150, '됐을': 151, '오리가미': 152, '좋은': 153, '드리퍼': 154, '파스텔': 155, '8': 156, '정도의': 157, '50': 158, '그런': 159, '아니면': 160, '맛있게': 161, '1차': 162, '처음에': 163, '미리': 164, '진행하도록': 165, '추출은': 166, '드릴': 167, '레시피로': 168, '레시피입니다': 169, '겁니다': 170, '스위치': 171, '확인을': 172, '얼음이': 173, '드시면': 174, '푸어해': 175, '양은': 176, '같습니다': 177, '워시드': 178, '최대한': 179, '진행하겠습니다': 180, '스터를': 181, '바리스타': 182, '있고요': 183, '작은': 184, '생각해': 185, '굵게': 186, '다른': 187, '단맛이': 188, '어떤': 189, '네': 190, '경우는': 191, '얼음을': 192, '추천을': 193, '드립니다': 194, '싱글': 195, '사용하는': 196, '주시는': 197, '그러면': 198, '되고': 199, '똑같이': 200, '이때': 201, '마지막으로': 202, '저는': 203, '10초': 204, '원두를': 205, '50g': 206, '따라서': 207, '사용하시면': 208, '방향으로': 209, '위해서': 210, '거고요': 211, '이번에': 212, '되고요': 213, '첫': 214, '필터의': 215, '커피입니다': 216, '있는데': 217, '추천드립니다': 218, '주겠습니다': 219, '필요한': 220, '60g의': 221, '분쇄도를': 222, '원두는': 223, '16g을': 224, 'v60': 225, '2분에서': 226, '원형으로': 227, '되었다면': 228, '집에서': 229, '뜸': 230, '번에': 231, '빠지면': 232, '제거하고': 233, '을': 234, '게이샤': 235, '시간이': 236, '내리시면': 237, '기다려': 238, '끝난': 239, '향이': 240, '하시면': 241, '이런': 242, '않도록': 243, '이용해서': 244, '똑같은': 245, '푸어를': 246, 'k': 247, '해주시고': 248, '시작하겠습니다': 249, '준비한': 250, '얼음잔에': 251, '네추럴': 252, '온도를': 253, '맛있는': 254, '필터는': 255, '드리려고': 256, '향을': 257, '드리퍼는': 258, '칼리타': 259, '마지막': 260, '약': 261, '안녕하십니까': 262, '여러분': 263, '에': 264, '냄새를': 265, '주셔서': 266, '레시피': 267, '정혜원입니다': 268, '플러스': 269, '마이너스': 270, '건데': 271, '18': 272, '효과를': 273, '40g': 274, '핫': 275, '16g의': 276, '1': 277, '비율은': 278, '좌우로': 279, '커피몽타주의': 280, '16': 281, '빠질': 282, '추출해': 283, '에티오피아': 284, '물론': 285, '사용한': 286, '거고': 287, '스위치가': 288, '온도가': 289, '내리는': 290, '딱': 291, '맞춰서': 292, '처음부터': 293, '드리퍼에': 294, '동일하게': 295, '마셔보도록': 296, '200g까지': 297, '산미가': 298, '가득': 299, '좋습니다': 300, '공유해': 301, '웨이브': 302, '가늘게': 303, '여러분들이': 304, '영상': 305, '시청해': 306, '두세': 307, '것보다는': 308, '주실': 309, '40초가': 310, '되거든요': 311, '에어': 312, '5': 313, '900': 314, '필터에': 315, '만들어': 316, '경우': 317, '간편하게': 318, '기준': 319, '향과': 320, '있게끔': 321, '점': 322, '사용하여': 323, '서버에서': 324, '확인하실': 325, '않고': 326, '추출하실': 327, '3분을': 328, '그다음': 329, '도로': 330, '완성입니다': 331, '진행을': 332, '해줄': 333, '4': 334, '부어주시면': 335, '있으니': 336, '보시면': 337, '빠른': 338, '안에': 339, '130g까지': 340, '표면을': 341, '센터': 342, '감사하겠습니다': 343, '있기': 344, '드실': 345, '방법은': 346, '보시고': 347, '흐름이': 348, '40g의': 349, '여러': 350, '잔에': 351, '30초에서': 352, '드리퍼와': 353, '결합을': 354, '양이': 355, '100g을': 356, '해주시고요': 357, '세게': 358, '취향에': 359, '와류를': 360, '같이': 361, '맛과': 362, '드리겠습니다': 363, '훨씬': 364, '빠르게': 365, '제거할': 366, '통해서': 367, '간단하게': 368, '950': 369, '960': 370, '만드는': 371, '매장에서는': 372, '추출에': 373, '타이머를': 374, '구형': 375, '성분이': 376, '1060': 377, '해줍니다': 378, '넣어줍니다': 379, '묵직한': 380, '미디움': 381, '사용합니다': 382, '15': 383, '잡고': 384, '여운을': 385, '사이에': 386, '해보도록': 387, '후에는': 388, '원도의': 389, '원형': 390, '해서': 391, '되게': 392, '일단': 393, '내리면서': 394, '린싱이': 395, '항상': 396, '가장': 397, '여름의': 398, '하는데': 399, '흑설탕': 400, '주입을': 401, '길게': 402, '60': 403, '추출하기': 404, '분쇄를': 405, '영': 406, '되면은': 407, '주세요': 408, '리브를': 409, '30': 410, '좋고요': 411, '풍부한': 412, '되시면': 413, '있지만': 414, '내려서': 415, '사용하실': 416, '로스팅': 417, '목표로': 418, '1분이': 419, '담겠습니다': 420, '9': 421, '를': 422, '느낌에': 423, '인제': 424, '완료가': 425, '동일한': 426, '베리베리': 427, '레이크': 428, '워시트': 429, '사용해': 430, '200g을': 431, '있게': 432, '내로': 433, '담기': 434, '좋겠습니다': 435, '다음': 436, '브르잉': 437, '전체': 438, '전에': 439, '유리': 440, '담아주세요': 441, '느끼실': 442, '편안하게': 443, '추출수를': 444, '30g의': 445, '거예요': 446, '3': 447, '2차': 448, '침출식': 449, '푸어하겠습니다': 450, 's': 451, '95도로': 452, '두고': 453, 'v60를': 454, '93도를': 455, '물의': 456, '수도': 457, '드리퍼와의': 458, '아주': 459, '살짝': 460, '추천드리고요': 461, '흔드는': 462, '주는': 463, '보니까': 464, '시간을': 465, '해주셔야': 466, '당연히': 467, '내려요': 468, '방지할': 469, '농장의': 470, '때에는': 471, '강한': 472, '충격을': 473, '것보다': 474, '흔들어주시는': 475, '끝나는': 476, '추출합니다': 477, '많은': 478, '50g을': 479, '바랍니다': 480, '빨리': 481, '해요': 482, '있으며': 483, '마무리하겠습니다': 484, '와인': 485, '단맛': 486, '깨끗한': 487, '드시는': 488, '보겠습니다': 489, '흡착되는': 490, '가져가시는': 491, '일반': 492, '약간': 493, '훌쩍': 494, '흐름을': 495, '위해서는': 496, '이연석입니다': 497, '70g': 498, '2차와': 499, '80g': 500, '이게': 501, '3분에서': 502, '이유는': 503, '산미와': 504, '맛': 505, '예열이': 506, '가지': 507, '넣어서': 508, '추가적으로': 509, '부어주도록': 510, '데': 511, '싶으시면': 512, '도는': 513, '밸런스와': 514, '만들어진': 515, '안': 516, '사용을': 517, '거기': 518, '올라가져': 519, '이재연입니다': 520, '큰': 521, '내': 522, '엄청': 523, '도달을': 524, '것이': 525, '93도로': 526, '올려서': 527, '사용하게': 528, '있다는': 529, '폰트': 530, '클래버': 531, '쓸': 532, '시간': 533, '린싱': 534, '가는': 535, '담긴': 536, '녹을': 537, '저어주세요': 538, '93도의': 539, '디카페인': 540, '레시피와': 541, '크게': 542, '분쇄': 543, '수위를': 544, '이로': 545, '거를': 546, '사용': 547, '알려드리도록': 548, '버전': 549, '위에': 550, '서버를': 551, '양도': 552, '시간에': 553, '뒤에': 554, '마찬가지로': 555, '접어주신': 556, '뜨거운': 557, '알려드릴': 558, '마이크론을': 559, '가운데에서': 560, '바깥쪽으로': 561, '소개하겠습니다': 562, '산미나': 563, '클래버를': 564, '40': 565, '서버와': 566, '물은': 567, '옮겨주시면': 568, '가지의': 569, '다를': 570, '주셨을': 571, '추출된': 572, '완료됩니다': 573, '앞서': 574, '대로': 575, '넣어주도록': 576, '누구나': 577, '나서': 578, '수정과': 579, '아까': 580, '레시피에': 581, '투과식으로': 582, '준비해': 583, '분들을': 584, '정도를': 585, '즐기시면': 586, '드리퍼가': 587, '특히': 588, '더반': 589, '평소에': 590, '이것도': 591, '맞춰져': 592, '있을': 593, '일정하게': 594, '으로': 595, '상태에서': 596, '예열을': 597, '되었습니다': 598, '새콤한': 599, '구슬이': 600, '잔을': 601, '애는': 602, '느껴지고': 603, '드리고': 604, '산미': 605, '여유를': 606, '2분이': 607, '사용하겠습니다': 608, '40g으로': 609, '열': 610, '여름에': 611, '말고': 612, '알': 613, '96도로': 614, '타': 615, '리브가': 616, '긴': 617, '과도하게': 618, '커피웍스': 619, '자': 620, '온도': 621, '동시에': 622, '플라스틱': 623, '소재의': 624, '맞게': 625, '분들이': 626, '커피워스의': 627, '하는데요': 628, '15초에서': 629, '여러분들도': 630, '그래야': 631, '라이팅': 632, '펠로': 633, '10번': 634, '게이샤를': 635, '분쇄들을': 636, '부드럽고': 637, '향미': 638, '골고루': 639, '볼게요': 640, '로스터': 641, '수위가': 642, '정도에': 643, '15초': 644, '정도가': 645, '열대': 646, '95': 647, '11': 648, '마셔보겠습니다': 649, '추출하고': 650, '시작하도록': 651, '원두가': 652, '느리게': 653, '초콜릿의': 654, '커피에': 655, '스퍼를': 656, '100g의': 657, '사이즈': 658, '25': 659, '점이': 660, '펠트': 661, '사인용': 662, '목적으로': 663, '추출될': 664, '전으로': 665, '정도까지': 666, '15초가': 667, '200g': 668, '드리퍼입니다': 669, '투가식으로': 670, '200g의': 671, '지금은': 672, '투과식으로도': 673, '추출하시면': 674, '30초를': 675, '분세도': 676, '시작할': 677, '뜬': 678, '부어주시고': 679, '담아서': 680, '넘어서': 681, '된다면': 682, '추천드려요': 683, '번째는': 684, '100': 685, '담고': 686, '부어서': 687, '이후에': 688, '기준으로는': 689, '고': 690, '50초가': 691, '부어줄': 692, '인해': 693, '명확한': 694, '노트를': 695, '40초': 696, '게이샤나': 697, '드셨을': 698, '향미나': 699, '매장에서': 700, '흔': 701, '넣어주신': 702, '종료가': 703, '전에는': 704, '304': 705, '로스터스': 706, '송요민입니다': 707, '이후': 708, '나머지': 709, '끝내는': 710, '하실': 711, '왜냐하면': 712, '살리기': 713, '물줄기를': 714, '커피에서': 715, '추가로': 716, '시작해서': 717, '달라붙게끔': 718, '강하게': 719, '포인트': 720, '마이크론에서': 721, '서버의': 722, '주셔야': 723, '속도가': 724, '시간에는': 725, '드리도록': 726, '거의': 727, '같아요': 728, '뒤에는': 729, '따라': 730, '다르게': 731, '가져가고': 732, '가수는': 733, '적셔질': 734, '해주시면': 735, '250': 736, '2번': 737, '펠트커피': 738, '되는데': 739, '300': 740, '과테말라': 741, '설탕': 742, '느껴지고요': 743, '브레잉': 744, '5일입니다': 745, '물에': 746, '말씀드린': 747, '무산소': 748, '포인트는': 749, '93': 750, '레시피이며': 751, '진행할': 752, '5g을': 753, '섞일': 754, '선호하시는': 755, '위한': 756, '추가식으로': 757, '블랜드고요': 758, '영점이': 759, '떨어지지': 760, '하리오의': 761, '더욱더': 762, '식': 763, '내추럴': 764, '베를린': 765, '나무': 766, '막대를': 767, '쉽고': 768, '예': 769, '처음': 770, '올라가지': 771, '풍부하게': 772, '거': 773, '커피인': 774, '높은': 775, '풍미가': 776, '통해': 777, '도움이': 778, '낙차를': 779, '원두의': 780, '매장과': 781, '부어줍니다': 782, '60g을': 783, '코스타리카': 784, '19g이고요': 785, '느껴져서': 786, '93도': 787, '계열': 788, '복합성이': 789, '나와서': 790, '청장하고': 791, '리브와': 792, '편이': 793, '영상에는': 794, '라인업': 795, '중에': 796, '센터커피': 797, '원두들을': 798, '끌어낼': 799, '94도로': 800, '설정하겠습니다': 801, '185인데요': 802, '번째와': 803, '힘드시다면': 804, '하셔도': 805, '상관없어요': 806, '300g을': 807, '채워주시면': 808, '요즘': 809, '계시는': 810, '어': 811, 's로': 812, '해보고': 813, '교환을': 814, '아이스로': 815, '어느': 816, '내렸다고': 817, '생각하시면': 818, '기다리시지': 819, '빼셔도': 820, '서버에는': 821, '중량으로': 822, '중량이': 823, '오바돼도': 824, '괜찮으니까': 825, '넣어주시면': 826, '대비': 827, '편입니다': 828, '하지만': 829, '입자는': 830, '진행하는': 831, '걸로': 832, '바리스타들이': 833, '1초에': 834, '것으로': 835, '기준을': 836, '삼고': 837, '되었으니': 838, '우리다': 839, '확보를': 840, '가이드를': 841, '보여드리려고': 842, '일단은': 843, 's고요': 844, '브라운': 845, '된': 846, '들여야': 847, '내렸을': 848, '때가': 849, '110g을': 850, '96도가': 851, '도달이': 852, '시작하신': 853, '50g까지': 854, '보어줍니다': 855, '타이머': 856, '시작과': 857, '사용하며': 858, '콜롬비아': 859, '톨리마': 860, '지역에': 861, '위치한': 862, '라팔마': 863, '게이샵': 864, '워시드입니다': 865, '잔의': 866, '커피몽타주에서는': 867, '20초': 868, '해드리고': 869, '먹어보도록': 870, '5g의': 871, '그라인더에': 872, '볼': 873, '시작하면서': 874, '60g까지': 875, '오늘은': 876, '20초가': 877, '클레버': 878, '분세는': 879, 'kk': 880, '5로': 881, '분쇄하도록': 882, '연말': 883, '하시길': 884, '원두량을': 885, '들리는': 886, '내려갈': 887, '초반에': 888, '유의해': 889, '로스팅의': 890, '250g을': 891, '도징량은': 892, '16g이며': 893, '내외로': 894, '용량은': 895, '적합한': 896, '용량이라고': 897, '생각하여서': 898, '숙성된': 899, '검포도에': 900, '플레이버': 901, '꿀과': 902, '라인과': 903, '깔끔하고': 904, '입안에': 905, '먹금': 906, '고서': 907, '골르시면': 908, '드리포트는': 909, '94도입니다': 910, '시음해': 911, '18g의': 912, '물량': 913, '270': 914, '이용한': 915, '누르시고': 916, '물줄기': 917, '속도를': 918, '얇게': 919, '가져가시거나': 920, '추천드리지': 921, '부어준': 922, '저어주도록': 923, '배전도가': 924, '배전입니다': 925, '원활하게': 926, '균형감': 927, '표현을': 928, '극대화하기': 929, '섞어준다는': 930, '느낌으로': 931, '80': 932, '3차도': 933, '푸어합니다': 934, '커뮤니티에': 935, '거기서': 936, '대략적으로': 937, '채웠어요': 938, '브르잉은': 939, '어떤지': 940, '먹어볼까요': 941, '흔들어': 942, '주신': 943, '먹었을': 944, '망고나': 945, '구아바': 946, '과일이': 947, '떠오르고요': 948, '했고': 949, '뒤집어서': 950, '빼주셔야': 951, '근데': 952, '복합적인': 953, '과일': 954, '앞부분을': 955, '뽑아낼': 956, '만들어지고': 957, '170g까지': 958, '맞춘': 959, '220g까지': 960, '다섯': 961, '섞어주세요': 962, '담도록': 963, '820': 964, '완료되었다면은': 965, '기다려둔': 966, '되었다면은': 967, '150g까지': 968, '150g이': 969, '궁금해하시는': 970, '분들': 971, '낙차의': 972, '높이를': 973, '가져가냐': 974, '보시면서': 975, '아시겠지만': 976, '원두들보다': 977, '진행하게': 978, '텁텁함': 979, '드리자면': 980, '시럽을': 981, '먹어볼게요': 982, '120g': 983, '높게': 984, '나오길': 985, '원하시면': 986, '94도의': 987, '2개시고': 988, '추수시간': 989, '처음이': 990, '110g': 991, '쓸게요': 992, '기본적인': 993, '클린컵에': 994, '중점을': 995, '뉘앙스들을': 996, '빠르면': 997, '하시고': 998, '사용량은': 999, '해줄게요': 1000, '분세도는': 1001, '로': 1002, 'top': 1003, '궁금하신': 1004, '있으시면': 1005, '댓글': 1006, '남겨주시면': 1007, '답글': 1008, '달아드리겠습니다': 1009, '마크': 1010, '기준에': 1011, '클릭': 1012, '그라인더마다': 1013, '다르다': 1014, '말씀드리지만': 1015, '확보가': 1016, '되지': 1017, '않으실': 1018, '이유는요': 1019, '크기는': 1020, '용의점이': 1021, '내려가지': 1022, '방지하는': 1023, '맛까지': 1024, '짧아요': 1025, '그리시면서': 1026, '200g이': 1027, '해야': 1028, '보여줄': 1029, '35g': 1030, '34': 1031, '초간': 1032, '8초': 1033, '커피웍스가': 1034, '내리는지': 1035, '맞췄습니다': 1036, '후미에는': 1037, '카나멜과': 1038, '43g에': 1039, '했어요': 1040, '80g이': 1041, '들어가니까': 1042, '장점이': 1043, '반갑습니다': 1044, '양태은입니다': 1045, '분쇄도와': 1046, '비슷해집니다': 1047, '월픽에서': 1048, '만나보실': 1049, '베아비스타': 1050, '게이사': 1051, '워시드인데요': 1052, '참고하신': 1053, '포인트에': 1054, '230g': 1055, '내렸습니다': 1056, '보온': 1057, '효과와': 1058, '덜기': 1059, '작업을': 1060, '시작할게요': 1061, '상큼한': 1062, '그거를': 1063, '진행하기': 1064, '물줄기로': 1065, '부어주시면서': 1066, '미분이': 1067, '나올': 1068, '가능성이': 1069, '그라인더는': 1070, '겉에': 1071, '나온': 1072, '것만': 1073, '씁니다': 1074, '빠졌으면': 1075, '저': 1076, '중시하기': 1077, '흔코리': 1078, '맡아서': 1079, '내림새를': 1080, '맡아': 1081, '잔에다': 1082, '가져가': 1083, '보시는': 1084, '달라붙어': 1085, '있어야지만': 1086, '빨라지기': 1087, '중요하게': 1088, '여기는': 1089, '시간이에요': 1090, '뜸들이기입니다': 1091, '이내로': 1092, '33g까지': 1093, '추가하도록': 1094, '맞춰주고요': 1095, '온도로': 1096, '브그': 1097, '레시피에서': 1098, '설정해': 1099, '마무리되었습니다': 1100, '이철원입니다': 1101, '내추럴이나': 1102, '허니': 1103, '낮아져요': 1104, '물줄기도': 1105, '빨라지거든요': 1106, '단테': 1107, '80g의': 1108, '높여서': 1109, '운동': 1110, '에너지를': 1111, '증가시켜서': 1112, '추출력을': 1113, '높여줄': 1114, '결과물은': 1115, '컵': 1116, '55초에': 1117, '끝났습니다': 1118, '필터나': 1119, '사용하시지만': 1120, '185': 1121, '필터고': 1122, '흰색이고요': 1123, '아이스와': 1124, '사컵은': 1125, '분세도를': 1126, '블랜딩': 1127, '진짜': 1128, '엄청나네요': 1129, '220g에서': 1130, '240g': 1131, '완료되어서': 1132, '컵의': 1133, '5개': 1134, '코스타리카나': 1135, '중인': 1136, '프르티': 1137, '후르티': 1138, '에서는요': 1139, '맞춰주면': 1140, '씨스': 1141, '0': 1142, '청주': 1143, '미원면': 1144, '수도원': 1145, '사용하시고': 1146, '평창수를': 1147, '넣어주시고': 1148, '세틀링을': 1149, '30조': 1150, '가': 1151, '10초가': 1152, '70': 1153, '빠졌을': 1154, '때쯤': 1155, '2차를': 1156, '넣은': 1157, '채': 1158, '소개할': 1159, '들여주시면': 1160, '많아지기': 1161, '많아져야': 1162, '대해': 1163, '언급해': 1164, '드린': 1165, '할게요': 1166, '훌륭한': 1167, '지나고': 1168, '내려주시면': 1169, '뒤': 1170, '추': 1171, '내리겠습니다': 1172, '맛있겠는데': 1173, '하냐면': 1174, '담았기': 1175, '뜸들이는': 1176, '많아지고요': 1177, '끝나면': 1178, '넣어줄': 1179, '냉각을': 1180, '시켜줍니다': 1181, '생각했을': 1182, '향': 1183, '톤이': 1184, '나오지': 1185, '않을까': 1186, '고인': 1187, '그때': 1188, '빼주시면': 1189, '좋을': 1190, '1100': 1191, '3분이': 1192, '준비된': 1193, '내려주도록': 1194, '준비하신': 1195, '옮겨주실': 1196, '푸어가': 1197, '시점은': 1198, '이내입니다': 1199, '정도인데요': 1200, '끝났으면': 1201, '결합해': 1202, '올리셔서': 1203, '확인해': 1204, 't': 1205, '빨라요': 1206, '듯하지만': 1207, '재미있는': 1208, '이용할': 1209, '사용하였고': 1210, '대략': 1211, '떨어지고': 1212, '생각보다': 1213, '이건': 1214, '250g': 1215, '필요할': 1216, '완료되셨다면': 1217, '웍스': 1218, '해주시게': 1219, '다들': 1220, '영상을': 1221, '모두': 1222, '되었으면': 1223, '버리고': 1224, '아이': 1225, '5초': 1226, '예상하시면': 1227, '2초': 1228, '끝이': 1229, '났습니다': 1230, '들일': 1231, '300g': 1232, '내립니다': 1233, '센터푸어를': 1234, '담은': 1235, '하게': 1236, '있는데요': 1237, '온도도': 1238, '그런데': 1239, '녹는': 1240, '4분의': 1241, '채우시고': 1242, '여러분들': 1243, '섬세하게': 1244, '때보다': 1245, '시원시원하게': 1246, '나타내고자': 1247, '특징을': 1248, '초경에': 1249, '추가해서': 1250, '300g의': 1251, '260g': 1252, '사이로': 1253, '파나마산': 1254, '토스게이샤': 1255, '180': 1256, '섞어줍니다': 1257, '40초로': 1258, '나왔고요': 1259, '설명드린': 1260, '케믹스': 1261, '날': 1262, '40초에서': 1263, '50초': 1264, '사이가': 1265, '추출되셨다면': 1266, '미리까지': 1267, '파카마라': 1268, '목적이었어요': 1269, '내려주는데': 1270, '내려가기': 1271, '부어주고요': 1272, '5g': 1273, '추천드리는': 1274, '1g': 1275, '사용하셔': 1276, '19g': 1277, '블랜딩을': 1278, '보여드릴게요': 1279, '저울': 1280, '디게싱은': 1281, '최소': 1282, '3일인데': 1283, '권장은': 1284, '담아주시면': 1285, '완료입니다': 1286, '해주는': 1287, '것도': 1288, '충분하게': 1289, '적셔서': 1290, '필터가': 1291, '예열한': 1292, '담으시면': 1293, '출은': 1294, '130부터': 1295, '커피들은': 1296, '보여드렸던': 1297, '않게끔': 1298, '김환입니다': 1299, '소개해드릴': 1300, '영상은': 1301, '레시피구요': 1302, '90g': 1303, '라이트': 1304, '포인트입니다': 1305, '향미를': 1306, 'v60을': 1307, '했습니다': 1308, '클린컵을': 1309, '중심으로': 1310, '통한': 1311, '여과식': 1312, '블랜딩하여': 1313, '과일의': 1314, '담았습니다': 1315, '이번에는': 1316, '내려져': 1317, '커피와': 1318, '수에': 1319, '접촉시간을': 1320, '늘려서': 1321, '용이하게': 1322, '만들어줍니다': 1323, '섞어주신': 1324, '바디감의': 1325, '스월링을': 1326, '해주신': 1327, '아이스컵에': 1328, '채워주신': 1329, '드셔주시면': 1330, '지났을': 1331, '블랜딩이에요': 1332, '12월': 1333, '월픽': 1334, '커피로': 1335, '사랑하시는': 1336, '연인': 1337, '친구': 1338, '가족분들과': 1339, '커피보다': 1340, '트러': 1341, '아이스커피에': 1342, '어울린다고': 1343, '생각하거든요': 1344, '인용': 1345, '바이패스하여': 1346, '비율이': 1347, '15입니다': 1348, '사용하였습니다': 1349, '진행하며': 1350, '것에': 1351, '막아주는': 1352, '넣고': 1353, '진행하고': 1354, '레시필러': 1355, '접어주시고': 1356, '예열해': 1357, '모드로': 1358, '가져다': 1359, '45초까지': 1360, '언스페셜티와': 1361, '비율': 1362, '실럽': 1363, '흙설탕': 1364, '2에': 1365, '제조를': 1366, '사용하시는': 1367, '나눠서': 1368, '추출하시는': 1369, '분들에게는': 1370, '방법을': 1371, '권장해': 1372, '나눠붙게': 1373, '지연이': 1374, '길어집니다': 1375, '800': 1376, '마이크롬에서': 1377, '간단하죠': 1378, '브루윙': 1379, '33초가': 1380, '1차를': 1381, '10번의': 1382, '남았고요': 1383, '파지법이라고': 1384, '자세도': 1385, '따지지': 1386, '않고요': 1387, '찬을': 1388, '부어줄게요': 1389, '하신': 1390, '얼음에': 1391, '희석해': 1392, '수의': 1393, '나': 1394, '하셨다면': 1395, '얼음과': 1396, '음료가': 1397, '제대로': 1398, '섞여': 1399, '돌려서': 1400, '에어에스': 1401, '캐릭터들이': 1402, '나타나냐면': 1403, '밝게': 1404, '다가오고': 1405, '160g': 1406, '30초의': 1407, '내에': 1408, '부으실': 1409, '터치하지': 1410, '3회': 1411, '추천해': 1412, '사용하시던': 1413, '하류': 1414, '즐겨': 1415, '드셨다면': 1416, '처음과': 1417, '주는데': 1418, '자가를': 1419, '해놓고': 1420, '방향보다는': 1421, '아래쪽으로': 1422, '잡아서': 1423, '일정량이': 1424, '끝내신': 1425, '35초': 1426, '들여주도록': 1427, '모든': 1428, '원두들은': 1429, '15초지만': 1430, '드리퍼로': 1431, '묵직하고': 1432, '깔끔한': 1433, '만들': 1434, '추출수로': 1435, '효과도': 1436, '제': 1437, '일정': 1438, '양에서': 1439, '이상': 1440, '않은': 1441, '계속': 1442, '내려주는': 1443, '95도의': 1444, '온수로': 1445, '추출하며': 1446, '보라색': 1447, '계열의': 1448, '체일이': 1449, '로우': 1450, '카페인인데': 1451, '단맛을': 1452, '느끼게': 1453, '완료를': 1454, '준': 1455, '흔들어주고요': 1456, '전': 1457, '넣어주면': 1458, '유속이': 1459, '조합이라고': 1460, '생각해서': 1461, '선택하게': 1462, '그람을': 1463, '설정했습니다': 1464, '코만단테': 1465, '26': 1466, '클릭입니다': 1467, '린칭을': 1468, '타이머에': 1469, '박상호입니다': 1470, '19g입니다': 1471, '군세도는': 1472, '11입니다': 1473, '붉은': 1474, '체리에서': 1475, '느껴지는': 1476, '새콤달콤함도': 1477, '장착하시고': 1478, '커피몽타주에서': 1479, '로스팅하고': 1480, '우간다': 1481, '음': 1482, '발레': 1483, '커피는요': 1484, '먹자마자': 1485, '좌두나': 1486, '딸기의': 1487, '선명하게': 1488, '따로': 1489, '45초부터': 1490, '조그마한': 1491, '거죠': 1492, '들여줄게요': 1493, '50g의': 1494, '요거도': 1495, '월요일': 1496, '투가식으로도': 1497, '장점을': 1498, '되어': 1499, '빠짐을': 1500, '방지해서': 1501, '워시드인데': 1502, '위치에서': 1503, '1차와': 1504, '3차는': 1505, '굵고': 1506, '교반이': 1507, '물주입의': 1508, '품종은': 1509, '아라모사라는': 1510, '품종인데': 1511, '카페인의': 1512, '함량이': 1513, '적다고': 1514, '줍니다': 1515, '의': 1516, '비율입니다': 1517, '알려드릴게요': 1518, '20g입니다': 1519, '빨라지고': 1520, '느려지고': 1521, '것들은': 1522, '감안해서': 1523, '코만': 1524, '전부': 1525, '고정된': 1526, '값으로': 1527, '분쇄하겠습니다': 1528, '일정한': 1529, '중심에서': 1530, '원을': 1531, '그리듯': 1532, '주며': 1533, '추출하는': 1534, '교반에': 1535, '자두나': 1536, '포도': 1537, '카카오니스의': 1538, '쌉쌀한': 1539, '후미도': 1540, '특이한': 1541, '평소보다': 1542, '많아요': 1543, '커피몽타주': 1544, '210g으로': 1545, '권장': 1546, '민싱이': 1547, '1000': 1548, '해주실': 1549, '리브의': 1550, '종이가': 1551, '밀착될': 1552, '환경마다': 1553, '겨울이': 1554, '다가왔고': 1555, '주변': 1556, '환경도': 1557, '낮아지기': 1558, '드리퍼도': 1559, '그렇고': 1560, '예열를': 1561, '들릴': 1562, '10초의': 1563, '보어서': 1564, '270g까지': 1565, '맞춰줄게요': 1566, '주도록': 1567, '밸런스과': 1568, '조화를': 1569, '이루어서': 1570, '달라집니다': 1571, '케멕스': 1572, '자세히': 1573, '설명해': 1574, '정말': 1575, '했으면': 1576, '보면': 1577, '향미적인': 1578, '요소들이': 1579, '덜': 1580, '에르바주산': 1581, '로켓': 1582, '네츄럴': 1583, '포도와': 1584, '와인을': 1585, '먹금은': 1586, '거와': 1587, '플레이버와': 1588, '오렌지와': 1589, '달콤한': 1590, '내부에는': 1591, '즐기실': 1592, '알려드린': 1593, '내려드셔': 1594, '여러분만의': 1595, '레시피도': 1596, '보시길': 1597, '커피답게': 1598, '자스민': 1599, '아로마가': 1600, '올라오고요': 1601, '갈색': 1602, '화이트': 1603, '스파클링한': 1604, '느낌이': 1605, '애프터의': 1606, '꿀': 1607, '질감의': 1608, '3차를': 1609, '280g까지': 1610, '두꺼운': 1611, '표면이': 1612, '평평해질': 1613, '헌이나': 1614, '싱글로리징': 1615, '브로잉이': 1616, '접촉될': 1617, '접어주시는': 1618, '내려드실': 1619, '오리진': 1620, '와류': 1621, '온도들이': 1622, '올라가기': 1623, '추출적인': 1624, '면모에서': 1625, '안정성이': 1626, '올라가서': 1627, '추출력이': 1628, '올라갑니다': 1629, '구운': 1630, '국물에': 1631, '씁쓸하고': 1632, '고소한': 1633, '호미가': 1634, '이어지고': 1635, '있네요': 1636, '어울릴': 1637, '만한': 1638, '드시기': 1639, '밝고': 1640, '청량한': 1641, '따지면': 1642, '타겟팅을': 1643, '식스트를': 1644, '핸터커피': 1645, '에시피를': 1646, 'v60같은': 1647, '마이크롬을': 1648, '친지식으로': 1649, '예어을': 1650, '보이식스를': 1651, '자금원형으로': 1652, '95도에': 1653, '도달했으면': 1654, '원형을': 1655, '그리면서': 1656, '40g까지': 1657, '부워줍니다': 1658, '협착되는': 1659, '16g이': 1660, '필요합니다': 1661, '레시피의': 1662, '본인': 1663, '세팅을': 1664, '잡아주시면': 1665, '코어링': 1666, '브르익': 1667, '레스트를': 1668, '쓰기': 1669, '섞어주지': 1670, '않으면': 1671, '추출해보겠습니다': 1672, '우선': 1673, '푸어는': 1674, '가져보도록': 1675, '산미에': 1676, '치중하는': 1677, '편이다': 1678, '보니': 1679, '초기에': 1680, '부분은': 1681, '만들려고': 1682, '적당한': 1683, '농도': 1684, '핵미들과': 1685, '조직이': 1686, '단단하기': 1687, '입자도': 1688, '곱게': 1689, '군쇄를': 1690, '하셔야': 1691, '여러분들의': 1692, '되기': 1693, '인용에': 1694, '160g의': 1695, '부어': 1696, '추칠': 1697, '레시피인': 1698, '16초': 1699, '완료하시는': 1700, '파이머를': 1701, '오리진보다는': 1702, '넘는': 1703, '걸렸다면은': 1704, '만나면': 1705, '접촉을': 1706, '줄이고': 1707, '공기의': 1708, '초화하겠습니다': 1709, '종일하게': 1710, '프': 1711, '언스테셜틱': 1712, '가시면': 1713, '나와요': 1714, '참고하주시면': 1715, '푸어링': 1716, '가지가': 1717, '부어졌을': 1718, '타겟이고': 1719, '제희가': 1720, '도징을': 1721, '도입니다': 1722, '갈아올': 1723, '잊지': 1724, '말아주세요': 1725, '5g이고요': 1726, '43s': 1727, '17입니다': 1728, '해줘요': 1729, '보통은': 1730, '매력적은인': 1731, '뉘앙스들인데': 1732, '부으신': 1733, '19g을': 1734, '마이크롬': 1735, '다크': 1736, '로스팅으로도': 1737, '소개하고': 1738, '구워주시고': 1739, '센조포': 1740, '센터포를': 1741, '부드러운': 1742, '바닐라의': 1743, '향미와': 1744, '밀크': 1745, '받춰주고': 1746, '필터입니다': 1747, '유의할': 1748, '점은': 1749, '리앙패를': 1750, '우리가': 1751, '거냐': 1752, '저으시거나': 1753, '외벽': 1754, '면을': 1755, '긁으시면서': 1756, '되셨을': 1757, '불쾌한': 1758, '들린': 1759, '포어는': 1760, '지나면': 1761, '크기에': 1762, '알에서': 1763, '담아주셔도': 1764, '괜찮습니다': 1765, '나오게끔': 1766, '타임을': 1767, '가져가시게끔': 1768, '18그람을': 1769, '민싱을': 1770, '기다려주며': 1771, '4분이': 1772, '제거해': 1773, '입자': 1774, '코': 1775, '만난': 1776, '테로는': 1777, '35초가': 1778, '예열되어서': 1779, '시피를': 1780, '또는': 1781, '쓴': 1782, '나무막대로': 1783, '레시티': 1784, '타게팅에서': 1785, '유공': 1786, '이는': 1787, '디팅': 1788, '랩스윗': 1789, '핫일': 1790, '5를': 1791, '쓰고': 1792, '브레잉을': 1793, '18g에': 1794, '물량을': 1795, '에시피에': 1796, '95도를': 1797, '흔들기보다는': 1798, '분세도나': 1799, '조절하여': 1800, '조절을': 1801, '조절하시면': 1802, '시작': 1803, '잔맛을': 1804, '도': 1805, '직량은': 1806, '출': 1807, '20g을': 1808, '코만단테는': 1809, '24클릭을': 1810, '85g까지': 1811, '도달': 1812, '무더운': 1813, '열대가일': 1814, '마': 1815, '즐겨주시면': 1816, '설지': 1817, '해체하시고': 1818, '리더에': 1819, '보여드릴': 1820, '브루윙의': 1821, '미리를': 1822, '라이스트': 1823, '동일합니다': 1824, '잡어서': 1825, '낮아진': 1826, '밀도': 1827, '미분': 1828, '발생이': 1829, '많아집니다': 1830, '튼들리기는': 1831, '군세도만': 1832, '디플레이크': 1833, '17g을': 1834, '245g': 1835, 'e': 1836, '2코만': 1837, '클릭으로': 1838, '예정입니다': 1839, '주전자': 1840, '세팅한': 1841, '96도입니다': 1842, '카페': 1843, '디게싱': 1844, '4일에서': 1845, '5일': 1846, '때부터': 1847, '진행하시는': 1848, '원스페셜티': 1849, '브루인': 1850, '쉽게': 1851, '말씀드리면': 1852, '대개': 1853, '종료되는데': 1854, '끝내주시면': 1855, '이쪽을': 1856, '에어를': 1857, '금': 1858, '오리가미가': 1859, '집에': 1860, '있으신': 1861, '분들은': 1862, '해주시는': 1863, '낼': 1864, '수위치를': 1865, '포화해': 1866, '포해': 1867, '블렌딩입니다': 1868, '접세지': 1869, '꿈을': 1870, '월픽을': 1871, '드셔보셨으면': 1872, '185g이': 1873, '되게끔': 1874, '하도록': 1875, '드리퍼에서': 1876, '필터지가': 1877, '뜨지': 1878, '붙여서': 1879, '사용해주시면': 1880, '플레이보가': 1881, '매력적이지만': 1882, '받쳐줘야': 1883, '밸런스': 1884, '잡히기': 1885, '5초가': 1886, '160g까지': 1887, '커피마다': 1888, '특색을': 1889, '디테일까지': 1890, '신경을': 1891, '썼고요': 1892, '하시지': 1893, '하셔서': 1894, '충린싱': 1895, '브루잉이': 1896, '내린': 1897, '내려오면': 1898, '그라인더를': 1899, '쓰시던': 1900, '잡으시고': 1901, '좋으실': 1902, '린싱하실': 1903, '옆면이': 1904, '무너지지': 1905, '정중앙에': 1906, '부우신': 1907, '옆면을': 1908, '린싱해': 1909, '나갔다가': 1910, '가운데로': 1911, '돌아오시면': 1912, '눌러서': 1913, '해주고요': 1914, '접으실': 1915, '활용할': 1916, '접어두시는': 1917, '예열되었다면': 1918, '브루잉에서': 1919, '페루': 1920, '싼': 1921, '세바스티안': 1922, '워시드를': 1923, '오유': 1924, '사용하셔서': 1925, '부드림': 1926, '브이식스티로': 1927, '월리고': 1928, '구어주고요': 1929, '이때부터는': 1930, '높아지지': 1931, '신경': 1932, '써주세요': 1933, '인싱이': 1934, '빨라지면': 1935, '살고': 1936, '길어지면': 1937, '좋아지는데': 1938, '파스타': 1939, '씨': 1940, '부십노우': 1941, '부': 1942, '136g': 1943, '내렸어요': 1944, '포화해줍니다': 1945, '17g에서': 1946, '18g': 1947, '부워주시고': 1948, '센터푸어': 1949, '프로세싱에': 1950, '45초가': 1951, '린싱은': 1952, '이재현입니다': 1953, '래서': 1954, '조사했을': 1955, '반종이': 1956, '내려들': 1957, '오리즌': 1958, '엘스코로': 1959, '유귀농': 1960, '딥플레이크': 1961, '문쇄한': 1962, '되는데요': 1963, '파나마': 1964, '아브게이샤': 1965, '어네어': 1966, '로빙': 1967, '이사': 1968, '아이스트로': 1969, '브로링': 1970, '원주량을': 1971, '점을': 1972, '잡으시고요': 1973, '92': 1974, '도의': 1975, '준비를': 1976, '뜸드리기를': 1977, '포어를': 1978, '시작해': 1979, '보도록': 1980, '센터포어를': 1981, '블루밍이': 1982, '깨지지': 1983, '유지를': 1984, '하려고': 1985, '편이에요': 1986, '더버': 1987, '멜리': 1988, '바스타': 1989, '물주입': 1990, '빠진': 1991, '물주입을': 1992, '물주입은': 1993, '19그람을': 1994, '투과식': 1995, '브르링': 1996, '아': 1997, '벤치마이': 1998, '추추를': 1999, '저어줄': 2000, '1인용에': 2001, '캔더포어를': 2002, '진행할수록': 2003, '여기': 2004, '원두들이': 2005, '추출되는': 2006, '온수의': 2007, '설정을': 2008, '에티오피아와': 2009, '콜롬비아를': 2010, '나라에서': 2011, '즐기는': 2012, '느낌을': 2013, '추출하게': 2014, '완료된': 2015, '추출물이': 2016, '바스푼을': 2017, '저어주신': 2018, '바테말라엘': 2019, '소코로': 2020, '따뜻하게': 2021, '내려보도록': 2022, '방법입니다': 2023, '언스페셜티에': 2024, '코만단테로는': 2025, '바디가의': 2026, '도를': 2027, '브라질': 2028, '다테아로': 2029, '카페인이라는': 2030, '원두입니다': 2031, '블랜딩입니다': 2032, '블링': 2033, '드립포트': 2034, '예열': 2035, '추추로': 2036, '개발한': 2037, '오래된': 2038, '미래라는': 2039, '블랜디입니다': 2040, '이면': 2041, '비율을': 2042, '같고요': 2043, '잔민을': 2044, '추추는': 2045, '추출수': 2046, '92도로': 2047, '사용하는데': 2048, '유지하고': 2049, 'v60이시거나': 2050, '펠로우': 2051, '22g': 2052, '비율이에요': 2053, '갈을게요': 2054, '잔': 2055, '드시면서': 2056, '지친': 2057, '일상에서': 2058, '잠깐': 2059, '가지시고': 2060, '즐기시기': 2061, '바라면서': 2062, '하도': 2063, '하루': 2064, '되세요': 2065, '좋으면서': 2066, '평창수': 2067, 'ic': 2068, '스': 2069, '0추천드립니다': 2070, '분쇄해서': 2071, '품종': 2072, '계량과': 2073, '다양하고': 2074, '복잡한': 2075, '가공': 2076, '방식이': 2077, '많아지고': 2078, '스티이시거나': 2079, '봄배': 2080, '네추럴입니다': 2081, '허니나': 2082, '들인': 2083, '바이패스에서': 2084, '실추를': 2085, '열교환을': 2086, '내려가는': 2087, '푸어해서': 2088, '적셔주시면': 2089, '영향을': 2090, '미치기': 2091, '때문입니다': 2092, '오': 2093, '스티시거나': 2094, '55초': 2095, '끝나거든요': 2096, '구어줍니다': 2097, '확인하시고': 2098, '타이머을': 2099, '누르면서': 2100, '일찍': 2101, '다소': 2102, '밑밋밋할': 2103, '상대적으로': 2104, '늦게': 2105, '불필요한': 2106, '아예': 2107, '메시피로': 2108, '개이사의': 2109, '특유': 2110, '뉘앙스를': 2111, '느낄': 2112, '같습니': 2113, '에티': 2114, '오펠': 2115, '사용하시게': 2116, '내실': 2117, '수위는': 2118, '여기서': 2119, '보이시겠지만': 2120, '부분에서': 2121, '않아요': 2122, '수위을': 2123, '방법의': 2124, '특이점이라고': 2125, '한다면': 2126, '발상지인': 2127, '예맹과': 2128, '에티오피아를': 2129, '농익은': 2130, '와이니': 2131, '안면서도': 2132, '초콜레티한': 2133, '향들을': 2134, '보어주시면': 2135, '랩스익': 2136, '5였지만': 2137, '브이시스티가': 2138, '브라인딩': 2139, '오겠습니다': 2140, '커피에서는': 2141, '2015년': 2142, '방문을': 2143, '처음으로': 2144, '지금까지': 2145, '다이렉트': 2146, '트레이드를': 2147, '이어오고': 2148, '만큼': 2149, '품질을': 2150, '서버는': 2151, '옮겨주시기': 2152, '저어줍니다': 2153, '됐고': 2154, '이때부터': 2155, '45g을': 2156, '센터푸어로': 2157, '220': 2158, '230': 2159, '내리셔도': 2160, '상관은': 2161, '없어요': 2162, '끝이에요': 2163, '줄기': 2164, '굵기': 2165, '자체도': 2166, '얇고': 2167, '높이에서': 2168, 'v50': 2169, '빼면': 2170, '포어로': 2171, '원두에': 2172, '가능한': 2173, '내려드릴': 2174, '건': 2175, '텐데요': 2176, '이때는': 2177, '안에서도': 2178, '내리시는': 2179, '방법에': 2180, '조금씩': 2181, '달라지시긴': 2182, '브르잉을': 2183, '위디움': 2184, '로스팅에서는': 2185, '분쇄도': 2186, '영에': 2187, '흰색': 2188, '브루잉을': 2189, '도달했을': 2190, '브로잉': 2191, '심플하면서도': 2192, '하기': 2193, '일관성': 2194, '결과를': 2195, '설계하였습니다': 2196, '바이패스해서': 2197, '언스페셜티': 2198, '가셔가지고': 2199, '카펙': 2200, '예열해줄': 2201, '라': 2202, '에르모사': 2203, '게이샤입니다': 2204, '타겟팅해서': 2205, '파파와': 2206, '파파야': 2207, '드': 2208, '컵을': 2209, '블린딩은': 2210, '보면은': 2211, '올': 2212, '주가를': 2213, '좋아하시는': 2214, '월피커': 2215, '만든': 2216, '같기도': 2217, '방을': 2218, '포화하겠습니다': 2219, '포': 2220, '일본': 2221, '245g까지': 2222, '포호해': 2223, '부워지는': 2224, '인해서': 2225, '가늘어진': 2226, '찐하게': 2227, '드시고': 2228, '괜찮다고': 2229, '생각해요': 2230, '크겠습니다': 2231, '린싱하는': 2232, '과정을': 2233, '필터와': 2234, '밀착하게': 2235, '일관성을': 2236, '유지하는': 2237, '쇠': 2238, '브루잉하는': 2239, '진행한': 2240, '70g을': 2241, '진행합니다': 2242, '해치하시고': 2243, '레몬과': 2244, '청사과': 2245, '산미로': 2246, '캔디': 2247, '달콤함이': 2248, '느껴집니다': 2249, '내려주실': 2250, '4회': 2251, '저어주시면': 2252, '4분': 2253, '30초로': 2254, '기': 2255, '가수해': 2256, '55초가': 2257, '하나의': 2258, '12': 2259, 'v60로': 2260, '클레버를': 2261, '보완하고자': 2262, '붉게': 2263, '분쇄하고': 2264, '주입은': 2265, '주입에서': 2266, '사': 2267, '레시필레': 2268, '원조의': 2269, '딥블루레이크': 2270, '드리포트': 2271, '레얼': 2272, '소개합니다': 2273, '드리퍼는요': 2274, '원포어로': 2275, '입자를': 2276, '분석을': 2277, '1030': 2278, '맞추시는': 2279, '02': 2280, '적셔지게': 2281, '들이는': 2282, '적셔주신': 2283, '45초': 2284, '기다리시고': 2285, '다음에는': 2286, '140g을': 2287, '구워주실': 2288, '가실': 2289, 'v60가': 2290, '것처럼': 2291, '준비물은': 2292, '16g': 2293, '뜸의': 2294, '주입하고': 2295, '원프어로': 2296, '120g을': 2297, '주입한': 2298, '끝내도록': 2299, '중요한': 2300, '비슷하게': 2301, '세팅': 2302, '레시피기': 2303, '수출을': 2304, '초가': 2305, '베를': 2306, '김야입니다': 2307, '뜸들이기를': 2308, '브레인': 2309, '올리기': 2310, '추천드린': 2311, '빠시면': 2312, '셔서': 2313, '저도': 2314, '맛을': 2315, '6트': 2316, '낙찰를': 2317, '낙수출하는': 2318, '언스펠티에': 2319, '실럽을': 2320, '넣으시면': 2321, '극대화되면서': 2322, '있던': 2323, '기존의': 2324, '향미들도': 2325, '40초까지': 2326, '백': 2327, '30g까지': 2328, '보어주시고': 2329, '푸어인': 2330, '불링하는': 2331, 'top아': 2332, '처음에는': 2333, '고루고루': 2334, '접시': 2335, '줘요': 2336, '하고서': 2337, '넘치지는': 2338, '끊어내면': 2339, '쓴맛이': 2340, '적절한': 2341, '속도감을': 2342, '오리감': 2343, '에어레스피': 2344, '추출양도': 2345, '리퍼의': 2346, '설장을': 2347, '게이사를': 2348, '올리즘보다는': 2349, '벤치마지': 2350, '게샤': 2351, '추르': 2352, '드립': 2353, '원도': 2354, '이현성입니다': 2355, '휘저으시거나': 2356, '외벽면을': 2357, '긁게': 2358, '불쾌함': 2359, '공정상': 2360, '밀도가': 2361, '낮아집니다': 2362, '디플루레이크': 2363, '아이스는': 2364, '핫이랑': 2365, '똑같아요': 2366, '대신': 2367, '달라지고요': 2368, '에르바지오': 2369, '살로케': 2370, '세미': 2371, '안토니오': 2372, '바란테스': 2373, '가족이': 2374, '운영하는': 2375, '농장입니다': 2376, '블랜딩은': 2377, '시즌': 2378, '블랜딩인데': 2379, '새롭게': 2380, '구성하여': 2381, '소개하게': 2382, '투가식이기': 2383, '이번엔': 2384, '내려가져': 2385, '브링': 2386, '포어링을': 2387, '유지해': 2388, '주면서': 2389, '성분': 2390, '이루어질': 2391, '포인트예요': 2392, '9입니다': 2393, '블랜드': 2394, '이름답게': 2395, '스파이시한': 2396, '계피': 2397, '올라오면서': 2398, '조첨': 2399, '반맛과': 2400, '질감이': 2401, '이름': 2402, '그대로': 2403, '느낌이고요': 2404, '흔들어주세요': 2405, '커피웍스의': 2406, '마실': 2407, '모금까지': 2408, '부담': 2409, '없이': 2410, '커피들을': 2411, '준비했습니다': 2412, '털지': 2413, '서버': 2414, '머그저울은': 2415, '필수고': 2416, '물입니다': 2417, '기다리시는': 2418, '백밀히': 2419, '두도록': 2420, '모아줄': 2421, '오목한': 2422, '해드리는데요': 2423, '워스의': 2424, '가에': 2425, '95도': 2426, '진행이': 2427, '진행하시면': 2428, '가진': 2429, '레몬그라스와': 2430, '블루베리': 2431, '그린티와': 2432, '경우도': 2433, '레시피처럼': 2434, '20초까지': 2435, '가주시면': 2436, '20초를': 2437, '넘지': 2438, '않으시는': 2439, '와료': 2440, '브이': 2441, '시스티': 2442, '전도율이': 2443, '낮아서': 2444, '데우는': 2445, '등의': 2446, '번거로움이': 2447, '적고': 2448, '블랜드는': 2449, '코스파리카': 2450, '카페대': 2451, '알뚜라': 2452, '저희분': 2453, '세도는': 2454, '단체': 2455, 't는': 2456, '마이크론이라고': 2457, '마이크는': 2458, '고운': 2459, '천': 2460, '18g을': 2461, '끝으로': 2462, '진자회일과': 2463, '향신료의': 2464, '시원하게': 2465, '더욱': 2466, '다채롭게': 2467, '다가오는': 2468, '19': 2469, '실험해': 2470, '예를': 2471, '들어서': 2472, '라이업에': 2473, '등등은': 2474, '그것보다': 2475, '빠를': 2476, '예열된': 2477, '브이식스티': 2478, '필터고요': 2479, '70g의': 2480, '담겨져': 2481, '전반적으로': 2482, '중이며': 2483, '브리스타': 2484, '92도에서': 2485, '출출할': 2486, '예정이며': 2487, '넣': 2488, '있어서': 2489, '올리셨을': 2490, '핸드커피': 2491, '200g입니다': 2492, '데비': 2493, '바이': 2494, '스타': 2495, '원스펠티': 2496, '웨이크': 2497, '리브에': 2498, '티': 2499, '세라믹을': 2500, '하리오에': 2501, 'v60시거나': 2502, '필타를': 2503, '선글게': 2504, '있습': 2505, '비해': 2506, '편인데': 2507, '했을': 2508, '포어인': 2509, '맛은': 2510, '무엇일까': 2511, '라는': 2512, '고민에': 2513, '답을': 2514, '찾으며': 2515, '개발하게': 2516, '원푸어로': 2517, '품질의': 2518, '편안하고': 2519, '레스피를': 2520, '만들었습니다': 2521, '리브는': 2522, '만들어줄': 2523, '투출양도': 2524, '구워지는': 2525, '번에서': 2526, '등수를': 2527, '가져가는': 2528, '빠르기': 2529, '적은': 2530, '양의': 2531, '넣었다': 2532, '분쇄도만': 2533, '감합니다': 2534, '902': 2535, '92도에': 2536, '준비가': 2537, '하시고요': 2538, '저울의': 2539, '영점과': 2540, '아이셀레이시피의': 2541, '처음이랑': 2542, '서': 2543, '버이는': 2544, '따주면': 2545, '타기팅을': 2546, '5입니다': 2547, '다크로스팅': 2548, '맛의': 2549, '88': 2550, '낮춰주시고': 2551, '수를': 2552}\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e545cdb-cd3e-4332-b29d-92b5e941006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[149, 252, 16, 2, 14, 787, 48, 193, 604, 10, 253, 1614, 178, 788, 2, 14, 452, 193, 194], [3, 103, 2, 14, 21, 2, 14, 1615, 2, 14], [3, 605, 3, 789, 361, 15, 790, 58, 791, 254, 29, 1616, 25, 8], [255, 792, 19, 1617, 1, 39, 26, 606, 453, 1618, 793, 300], [63, 21, 6, 1619, 1, 40, 51, 301, 256, 49, 27, 794, 20, 195, 1620, 795, 796, 454, 93, 135, 21, 797, 798, 179, 362, 257, 799, 1, 73, 51, 301, 363]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4316a638-bfec-467d-964b-d802db1fd6b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '저희는 네추럴 커피 같은 경우에는 93도 정도로 추천을 드리고, 물 온도를 헌이나 워시드 계열 같은 경우에는 95도로 추천을 드립니다.\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m----> 3\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m x_validation \u001b[38;5;241m=\u001b[39m pad_sequences(x_validation, maxlen\u001b[38;5;241m=\u001b[39mmax_len)\n\u001b[1;32m      5\u001b[0m x_test \u001b[38;5;241m=\u001b[39m pad_sequences(x_test, maxlen\u001b[38;5;241m=\u001b[39mmax_len)\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/keras/src/utils/data_utils.py:1132\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m trunc \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1138\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '저희는 네추럴 커피 같은 경우에는 93도 정도로 추천을 드리고, 물 온도를 헌이나 워시드 계열 같은 경우에는 95도로 추천을 드립니다.\\n'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_validation = pad_sequences(x_validation, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e60150ce-798f-4f0d-a0d3-3cfc370ee1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 60))\n",
    "model.add(SimpleRNN(60))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61880244-18fb-41c4-8958-76e15c1aca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, None, 60)          168120    \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 60)                7260      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175441 (685.32 KB)\n",
      "Trainable params: 175441 (685.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "674ddbd2-eaa6-46bf-9c28-e7709b1d4498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "13/13 [==============================] - 3s 142ms/step - loss: 0.6526 - acc: 0.6156 - val_loss: 0.6585 - val_acc: 0.6061\n",
      "Epoch 2/1000\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 0.5142 - acc: 0.8753 - val_loss: 0.5372 - val_acc: 0.7879\n",
      "Epoch 3/1000\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 0.2790 - acc: 0.9403 - val_loss: 0.4633 - val_acc: 0.7818\n",
      "Epoch 4/1000\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.1341 - acc: 0.9714 - val_loss: 0.4288 - val_acc: 0.7879\n",
      "Epoch 5/1000\n",
      "13/13 [==============================] - 1s 116ms/step - loss: 0.0829 - acc: 0.9779 - val_loss: 0.4463 - val_acc: 0.7939\n",
      "Epoch 6/1000\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 0.0530 - acc: 0.9818 - val_loss: 0.4717 - val_acc: 0.8182\n",
      "Epoch 7/1000\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 0.0444 - acc: 0.9883 - val_loss: 0.5137 - val_acc: 0.7879\n",
      "Epoch 8/1000\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0431 - acc: 0.9870 - val_loss: 0.4771 - val_acc: 0.8121\n",
      "Epoch 9/1000\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.0334 - acc: 0.9857 - val_loss: 0.5522 - val_acc: 0.8000\n",
      "Epoch 10/1000\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 0.0296 - acc: 0.9883 - val_loss: 0.5141 - val_acc: 0.8121\n",
      "Epoch 11/1000\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 0.0300 - acc: 0.9883 - val_loss: 0.5361 - val_acc: 0.8061\n",
      "Epoch 12/1000\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0263 - acc: 0.9883 - val_loss: 0.5524 - val_acc: 0.8061\n",
      "Epoch 13/1000\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 0.0231 - acc: 0.9896 - val_loss: 0.5749 - val_acc: 0.8000\n",
      "Epoch 14/1000\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 0.0240 - acc: 0.9896 - val_loss: 0.5417 - val_acc: 0.8061\n",
      "Epoch 15/1000\n",
      "13/13 [==============================] - 1s 52ms/step - loss: 0.0265 - acc: 0.9857 - val_loss: 0.5666 - val_acc: 0.8061\n",
      "Epoch 16/1000\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.0251 - acc: 0.9870 - val_loss: 0.5593 - val_acc: 0.8121\n",
      "Epoch 17/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0206 - acc: 0.9870 - val_loss: 0.5838 - val_acc: 0.8000\n",
      "Epoch 18/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0186 - acc: 0.9883 - val_loss: 0.5872 - val_acc: 0.8061\n",
      "Epoch 19/1000\n",
      "13/13 [==============================] - 1s 57ms/step - loss: 0.0205 - acc: 0.9909 - val_loss: 0.6088 - val_acc: 0.8061\n",
      "Epoch 20/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0213 - acc: 0.9870 - val_loss: 0.6203 - val_acc: 0.8061\n",
      "Epoch 21/1000\n",
      "13/13 [==============================] - 1s 52ms/step - loss: 0.0190 - acc: 0.9909 - val_loss: 0.6010 - val_acc: 0.8061\n",
      "Epoch 22/1000\n",
      "13/13 [==============================] - 1s 58ms/step - loss: 0.0186 - acc: 0.9896 - val_loss: 0.6112 - val_acc: 0.8061\n",
      "Epoch 23/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0214 - acc: 0.9857 - val_loss: 0.6049 - val_acc: 0.8121\n",
      "Epoch 24/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0176 - acc: 0.9909 - val_loss: 0.6231 - val_acc: 0.8121\n",
      "Epoch 25/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0175 - acc: 0.9896 - val_loss: 0.6244 - val_acc: 0.8121\n",
      "Epoch 26/1000\n",
      "13/13 [==============================] - 1s 53ms/step - loss: 0.0192 - acc: 0.9870 - val_loss: 0.6726 - val_acc: 0.8061\n",
      "Epoch 27/1000\n",
      "13/13 [==============================] - 1s 49ms/step - loss: 0.0182 - acc: 0.9857 - val_loss: 0.6170 - val_acc: 0.8061\n",
      "Epoch 28/1000\n",
      "13/13 [==============================] - 1s 48ms/step - loss: 0.0215 - acc: 0.9883 - val_loss: 0.5842 - val_acc: 0.8000\n",
      "Epoch 29/1000\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 0.0227 - acc: 0.9870 - val_loss: 0.6870 - val_acc: 0.8061\n",
      "Epoch 30/1000\n",
      "13/13 [==============================] - 1s 44ms/step - loss: 0.0185 - acc: 0.9844 - val_loss: 0.6156 - val_acc: 0.8121\n",
      "Epoch 31/1000\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 0.0175 - acc: 0.9909 - val_loss: 0.6241 - val_acc: 0.8182\n",
      "Epoch 32/1000\n",
      "13/13 [==============================] - 1s 43ms/step - loss: 0.0162 - acc: 0.9831 - val_loss: 0.6350 - val_acc: 0.8061\n",
      "Epoch 33/1000\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 0.0161 - acc: 0.9870 - val_loss: 0.6467 - val_acc: 0.8121\n",
      "Epoch 34/1000\n",
      "13/13 [==============================] - 1s 52ms/step - loss: 0.0151 - acc: 0.9909 - val_loss: 0.6355 - val_acc: 0.8061\n",
      "Epoch 35/1000\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 0.0169 - acc: 0.9857 - val_loss: 0.6622 - val_acc: 0.8182\n",
      "Epoch 36/1000\n",
      "13/13 [==============================] - 1s 62ms/step - loss: 0.0174 - acc: 0.9896 - val_loss: 0.6451 - val_acc: 0.8061\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience = 30)\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=60, validation_data = (x_validation, y_validation),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b830c5e-7362-4399-8fd7-6fb1b1fd2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5782 - acc: 0.8242\n",
      "\n",
      " 정확도: 0.8242\n"
     ]
    }
   ],
   "source": [
    "_loss, _accr = model.evaluate(x_test, y_test)\n",
    "print(\"\\n 정확도: %.4f\" % _accr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa28b57-f401-43f2-9955-08c2305cc97f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kobert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkobert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_pytorch_kobert_model\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([[\u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m99\u001b[39m], [\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      4\u001b[0m input_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kobert'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from kobert import get_pytorch_kobert_model\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "model, vocab  = get_pytorch_kobert_model()\n",
    "sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)\n",
    "pooled_output.shape\n",
    "vocab\n",
    " # Last Encoding Layer\n",
    "sequence_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4e28ab-6ebf-4093-9a76-6301d5a5b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8027911-dcb3-4ed0-b502-772fc48a6e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349f8759-cbd0-47e5-9c96-83018ba742d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a5b3d0-7a87-480a-b724-eba43691d2d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTSVDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_discard_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_separator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTSVDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.cache/ratings_test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, field_indices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m], num_discard_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/gluonnlp/data/dataset.py:136\u001b[0m, in \u001b[0;36mTSVDataset.__init__\u001b[0;34m(self, filename, encoding, sample_splitter, field_separator, num_discard_samples, field_indices, allow_missing)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_indices \u001b[38;5;241m=\u001b[39m field_indices\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_missing \u001b[38;5;241m=\u001b[39m allow_missing\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28msuper\u001b[39m(TSVDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/gluonnlp/data/dataset.py:160\u001b[0m, in \u001b[0;36mTSVDataset._read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_separator:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_missing:\n\u001b[0;32m--> 160\u001b[0m         samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_selector(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_separator(s)) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         selected_samples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/virtualEnv/coffee/lib/python3.10/site-packages/gluonnlp/data/dataset.py:160\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_separator:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_missing:\n\u001b[0;32m--> 160\u001b[0m         samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_field_selector(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_field_separator\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         selected_samples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"output.csv\", field_indices=[0,1], num_discard_samples=0, field_separator=',')\n",
    "dataset_test = nlp.data.TSVDataset(\".cache/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b2273563-0581-410b-9a15-a0f4f7f80c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"output_without_dquotes.csv\", num_discard_samples=1)\n",
    "dataset_train._field_separator = ','  # Set the field separator here\n",
    "dataset_train._field_indices = [0, 1]\n",
    "\n",
    "dataset_test = nlp.data.TSVDataset(\".cache/ratings_test.txt\", num_discard_samples=1)\n",
    "dataset_test._field_separator = ','  # Set the field separator here\n",
    "dataset_test._field_indices = [1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d00e57e-af58-4889-858c-43f13145a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"안녕하세요, 반갑습니다. 폰트 커피 양태은입니다.']\n",
      "['\",0']\n",
      "[['\"안녕하세요, 반갑습니다. 폰트 커피 양태은입니다.'], ['\",0'], ['\"이번에 제가 알려드릴 레시피는 아이스 브루윙, 커피 레시피입니다.'], ['\",0'], ['\"저희 브루윙 레시피의 중요한 점은 상대적으로 굵은 물줄기를 사용하시는 것과 추출 시간에 약 50% 이내로 푸어를 끝내는 것입니다.'], ['\",0'], ['\"조금 더 쉽게 말씀드리면 저희 커피는 대개 추출이 3분 전으로 종료되는데, 이때 푸어를 마지막으로 1분 30초에서 40초 정도에 끝내주시면 됩니다.'], ['\",0'], ['\"그럼 커피를 내리면서 자세히 설명해 드리겠습니다.'], ['\",0']]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[:10][0])\n",
    "print(dataset_train[:10][1])\n",
    "print(dataset_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b25a00b-6e17-4466-a64c-50d941abe2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face를 통한 모델 및 토크나이저 Import\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7558012-10fc-4263-80c1-b007ad5b3438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████| 535/535 [00:00<00:00, 3.58MB/s]\n",
      "pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████| 369M/369M [00:06<00:00, 56.8MB/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m KoBERTTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskt/kobert-base-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m bertmodel \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskt/kobert-base-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mBERTVocab\u001b[38;5;241m.\u001b[39mfrom_sentencepiece(tokenizer\u001b[38;5;241m.\u001b[39mvocab_file, padding_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# pre-trained 모델 가져오기\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9c8809-9f4e-466b-afb5-8463db782463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4177/3665400552.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace('\\n', '') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file with pandas\n",
    "df = pd.read_csv(\"output.csv\", quoting=1)  # quoting=1 handles quotes around fields\n",
    "\n",
    "# Remove double quotes from all columns\n",
    "df = df.applymap(lambda x: x.replace('\\n', '') if isinstance(x, str) else x)\n",
    "\n",
    "# # Save the modified DataFrame back to a CSV file\n",
    "df.to_csv(\"output_without_quotes.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d8e02-481b-4a9f-b90b-584a98646669",
   "metadata": {},
   "source": [
    "<h1>여기서부터 kobert시작</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb069e9-3464-4a4d-8a5d-d3846e6b28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b88863-7ee3-4062-b403-f59a3c910487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efd845c-7059-4cf3-a861-a57101a97548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "PATH=\"./\"\n",
    "#device - GPU 설정\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392f83c0-22ee-4881-8607-06c77471af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4a2bf8-4074-44ba-b284-a42f0f4d840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTSentenceTransform\n",
    "\n",
    "class BERTSentenceTransform:\n",
    "    r\"\"\"BERT style data transformation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : BERTTokenizer.\n",
    "        Tokenizer for the sentences.\n",
    "    max_seq_length : int.\n",
    "        Maximum sequence length of the sentences.\n",
    "    pad : bool, default True\n",
    "        Whether to pad the sentences to maximum length.\n",
    "    pair : bool, default True\n",
    "        Whether to transform sentences or sentence pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._pad = pad\n",
    "        self._pair = pair\n",
    "        self._vocab = vocab\n",
    "\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
    "\n",
    "        The transformation is processed in the following steps:\n",
    "        - tokenize the input sequences\n",
    "        - insert [CLS], [SEP] as necessary\n",
    "        - generate type ids to indicate whether a token belongs to the first\n",
    "        sequence or the second sequence.\n",
    "        - generate valid length\n",
    "\n",
    "        For sequence pairs, the input is a tuple of 2 strings:\n",
    "        text_a, text_b.\n",
    "\n",
    "        Inputs:\n",
    "            text_a: 'is this jacksonville ?'\n",
    "            text_b: 'no it is not'\n",
    "        Tokenization:\n",
    "            text_a: 'is this jack ##son ##ville ?'\n",
    "            text_b: 'no it is not .'\n",
    "        Processed:\n",
    "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
    "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "            valid_length: 14\n",
    "\n",
    "        For single sequences, the input is a tuple of single string:\n",
    "        text_a.\n",
    "\n",
    "        Inputs:\n",
    "            text_a: 'the dog is hairy .'\n",
    "        Tokenization:\n",
    "            text_a: 'the dog is hairy .'\n",
    "        Processed:\n",
    "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
    "            type_ids: 0     0   0   0  0     0 0\n",
    "            valid_length: 7\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        line: tuple of str\n",
    "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
    "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
    "            string: (text_a,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
    "        np.array: valid length in 'int32', shape (batch_size,)\n",
    "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer.tokenize(text_a)\n",
    "        tokens_b = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
    "                                    self._max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
    "        # pre-training and are added to the wordpiece embedding vector\n",
    "        # (and position vector). This is not *strictly* necessary since\n",
    "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        #vocab = self._tokenizer.vocab\n",
    "        vocab = self._vocab\n",
    "        tokens = []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The valid length of sentences. Only real  tokens are attended to.\n",
    "        valid_length = len(input_ids)\n",
    "\n",
    "        if self._pad:\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            # use padding tokens for the rest\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73e0403-a9c7-4cd0-ac1a-4e9577793140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output_without_quotes.csv\", quoting=1)\n",
    "\n",
    "data_list = []\n",
    "for q, label in zip(df['Sentence'], df['Label'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3f3d03-acc4-43c3-8a7a-817fe8379de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, random_state=0)\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81a074c-6ff1-4280-a7fb-b5aec3efdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTDataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int64(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da746e3e-6f76-4cf2-9d7b-dd2deb405d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세팅 파라미터\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 15\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  3e-5\n",
    "max_non_improving_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57a0084-147d-448d-bf66-b0694e601f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTClassifier\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2745dbf6-a2b4-4fcf-9b5a-60bc0943ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#정의한 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49aa7d73-82fc-48b9-babb-49cfec7c2264",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m) \u001b[38;5;241m*\u001b[39m num_epochs\n\u001b[1;32m      2\u001b[0m warmup_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(t_total \u001b[38;5;241m*\u001b[39m warmup_ratio)\n\u001b[1;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_step, num_training_steps\u001b[38;5;241m=\u001b[39mt_total)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f2d299-da56-4339-ba8f-36f0ea791239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26391d3-8b82-4a83-9a52-ea33acd6a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로더\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04516a2-d88c-47fd-8e05-ad4ec9568c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:01<00:11,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7049440741539001 train acc 0.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.5198863636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.7394201159477234 validation acc 0.6041666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:05,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.5762256979942322 train acc 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.7073863636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation loss 0.5379661917686462 validation acc 0.8854166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:05,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.4789164662361145 train acc 0.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.8352272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 validation loss 0.3910291790962219 validation acc 0.8211805555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.40012142062187195 train acc 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.8934659090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 validation loss 0.25545448064804077 validation acc 0.8194444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.29448074102401733 train acc 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train acc 0.9303977272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 validation loss 0.18132546544075012 validation acc 0.7934027777777778\n",
      "No improvement for 3 epochs. Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.27it/s]\n",
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:Precision = 0.6847, Recall = 1.0000, F1-Score = 0.8128\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7009943127632141 train acc 0.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.5071022727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.7014644742012024 validation acc 0.6614583333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:06,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.6070520877838135 train acc 0.640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.7414772727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation loss 0.425292044878006 validation acc 0.9201388888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:05,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.4732392728328705 train acc 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.8863636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 validation loss 0.19814452528953552 validation acc 0.8559027777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.3822648525238037 train acc 0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.8977272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 validation loss 0.2193126529455185 validation acc 0.9548611111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:05,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.3296426832675934 train acc 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train acc 0.8948863636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 validation loss 0.20181436836719513 validation acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.27216461300849915 train acc 0.890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train acc 0.9417613636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 validation loss 0.1487387716770172 validation acc 0.9201388888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.24685867130756378 train acc 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train acc 0.953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 validation loss 0.11977105587720871 validation acc 0.90625\n",
      "No improvement for 3 epochs. Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.73it/s]\n",
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2:Precision = 0.9610, Recall = 0.9737, F1-Score = 0.9673\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.3519638478755951 train acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.9446022727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.2554604709148407 validation acc 0.9791666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:06,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.22398412227630615 train acc 0.953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation loss 0.1603022813796997 validation acc 0.9409722222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.13833586871623993 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.9616477272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 validation loss 0.11809253692626953 validation acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.1414417028427124 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.9616477272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 validation loss 0.06448045372962952 validation acc 0.9375\n",
      "No improvement for 3 epochs. Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.99it/s]\n",
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3:Precision = 0.9367, Recall = 0.9737, F1-Score = 0.9548\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.5331048369407654 train acc 0.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.8465909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.4077312648296356 validation acc 0.984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:06,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.41766154766082764 train acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.9602272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation loss 0.17489494383335114 validation acc 0.9739583333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.1760384738445282 train acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.9644886363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 validation loss 0.07599371671676636 validation acc 0.9305555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.13217169046401978 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.9772727272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 validation loss 0.0822712853550911 validation acc 0.9357638888888888\n",
      "No improvement for 3 epochs. Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.26it/s]\n",
      "/home/woong/virtualEnv/coffee/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4:Precision = 0.9615, Recall = 0.9868, F1-Score = 0.9740\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.9151598811149597 train acc 0.109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.30113636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.618646502494812 validation acc 0.9722222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:06,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.5364415645599365 train acc 0.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation loss 0.1823844164609909 validation acc 0.9722222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.18404746055603027 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.9502840909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 validation loss 0.07726801931858063 validation acc 0.9791666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:04,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.1498614102602005 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.9573863636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 validation loss 0.07081420719623566 validation acc 0.9774305555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.15171226859092712 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train acc 0.9573863636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 validation loss 0.08975248783826828 validation acc 0.9704861111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▋                                                                            | 1/11 [00:00<00:03,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.15109407901763916 train acc 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train acc 0.9701704545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 validation loss 0.0818895623087883 validation acc 0.9600694444444445\n",
      "No improvement for 3 epochs. Early stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5:Precision = 0.9367, Recall = 0.9737, F1-Score = 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 X, y로 나누기\n",
    "X = [item[0] for item in dataset_train]\n",
    "y = [item[1] for item in dataset_train]\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# StratifiedKFold를 사용하여 인덱스 분할\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{kfold.get_n_splits()}\")\n",
    "\n",
    "    # 데이터 분할\n",
    "    train_data = [data_list[i] for i in train_idx]\n",
    "    val_data = [data_list[i] for i in val_idx]\n",
    "\n",
    "    # 데이터 로더\n",
    "    data_train = BERTDataset(train_data, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "    data_val = BERTDataset(val_data, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "    val_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    t_total = len(train_dataloader) * num_epochs\n",
    "    warmup_step = int(t_total * warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "    eval_acc = 0.0\n",
    "    train_loss_li = []\n",
    "    train_acc_li = []\n",
    "    val_loss_li = []\n",
    "    val_acc_li = []\n",
    "    non_improving_count = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        train_acc = 0.0\n",
    "        test_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "            loss = loss_fn(out, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_acc += calc_accuracy(out, label)\n",
    "\n",
    "            if batch_id % log_interval == 0:\n",
    "                print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "\n",
    "        print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "        train_loss_li.append(loss.data.cpu().numpy())\n",
    "        train_acc_li.append(train_acc / (batch_id+1))\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(val_dataloader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            test_acc += calc_accuracy(out, label)\n",
    "\n",
    "        print(\"epoch {} validation loss {} validation acc {}\".format(e+1, loss.data.cpu().numpy() ,test_acc / (batch_id+1)))\n",
    "\n",
    "        val_loss_li.append(loss.data.cpu().numpy())\n",
    "        val_acc_li.append(test_acc / (batch_id+1))\n",
    "\n",
    "        if test_acc > eval_acc:\n",
    "            eval_acc = test_acc\n",
    "            non_improving_count = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(model, PATH + f'checkpoint_fold{fold}.pt')  # 해당 fold의 모델 저장\n",
    "            torch.save(model.state_dict(), PATH + f'model_state_fold{fold}.pt')  # 해당 fold의 모델 state_dict 저장\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }, PATH + f'all_fold{fold}.tar')  # 해당 fold의 체크포인트 저장\n",
    "        else:\n",
    "            non_improving_count += 1\n",
    "\n",
    "        if non_improving_count >= max_non_improving_epochs:\n",
    "            print(f\"No improvement for {max_non_improving_epochs} epochs. Early stopping.\")\n",
    "            break\n",
    "\n",
    "    # 추가된 부분: 테스트 데이터에 대한 recall, f1-score 계산\n",
    "    model.load_state_dict(best_model_state)  # 저장된 최적의 모델로 복원\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        _, preds = torch.max(out, 1)\n",
    "\n",
    "        # 정확도 계산\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # recall, f1-score 계산\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "    print(f\"Fold {fold + 1}:Precision = {precision:.4f}, Recall = {recall:.4f}, F1-Score = {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bce6566f-e840-446e-a56c-ac747a9d06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH + 'checkpoint.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "model.load_state_dict(torch.load(PATH + 'model_state.pt'))  # state_dict를 불러 온 후, 모델에 저장\n",
    "\n",
    "checkpoint = torch.load(PATH + 'all.tar')   # dict 불러오기\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672048a8-e429-46e1-b803-88309270f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_result():\n",
    "    best_acc = 0\n",
    "    best_model = 0\n",
    "    for i in range(5):\n",
    "        model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "        model.load_state_dict(torch.load(PATH + f'model_state_fold{i}.pt'))\n",
    "        model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            # 모델 예측\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            _, preds = torch.max(out, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "        # recall, f1-score 계산\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        if accuracy>best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_model = i\n",
    "\n",
    "        print(f\"Fold {i + 1}:Precision = {precision:.4f}, Recall = {recall:.4f}, F1-Score = {f1:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# 예측 함수\n",
    "def predict(predict_sentence, best_model):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "    all_test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "    model.load_state_dict(torch.load(PATH + f'model_state_fold{best_model}.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(all_test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "# 행복, 놀람, 분노, 공포, 혐오, 슬픔, 중립\n",
    "    test_eval=[]\n",
    "    for i in out:\n",
    "        logits=i\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        if np.argmax(logits) == 0:\n",
    "            test_eval.append(\"아무것도 안\")\n",
    "        elif np.argmax(logits) == 1:\n",
    "            test_eval.append(\"추출이\")\n",
    "\n",
    "    print(test_eval[0] + \" 느껴집니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096ffbb-13ef-467c-90f6-e5dc66798cee",
   "metadata": {},
   "source": [
    "<h3>레시피와 관련된 문장을 분류하는지 확인해보기</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44df9e32-df93-4442-9511-701519e7670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:Precision = 0.8736, Recall = 0.8727, F1-Score = 0.8731, Accuracy = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2:Precision = 0.9697, Recall = 0.9682, F1-Score = 0.9684, Accuracy = 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3:Precision = 0.9737, Recall = 0.9727, F1-Score = 0.9729, Accuracy = 0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4:Precision = 0.9709, Recall = 0.9682, F1-Score = 0.9685, Accuracy = 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5:Precision = 0.9529, Recall = 0.9455, F1-Score = 0.9463, Accuracy = 0.9455\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  안녕하세요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아무것도 안 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  이번 추출에서는 하리오 v60을 사용하여 파나마 하트만 게이샤를 추출해보겠습니다\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출이 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  하리오 v60는 세라믹을 사용하고 있고 추출 전에 린싱과 예열을 해주는 것이 아주 중요합니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아무것도 안 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  이번 추출에서는 원두 20g을 사용하여 추출할 예정이고 1:15비율로 진행하겠습니다\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출이 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  처음에 추출을 시작하시면서 물을 푸어하기 전에 꼭 타이머를 먼저 시작해주셔야 합니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아무것도 안 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  처음에는 40초동안 40g의 물로 뜸을 들여주세요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출이 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  이후 40초가 되면 180g을 추가로 부어줍니다\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출이 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  마지막으로 1분 30초가 되면 나머지 80g을 추가로 추출해주겠습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출이 느껴집니다.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "추출 레시피를 입력해주세요 :  0\n"
     ]
    }
   ],
   "source": [
    "#질문 무한반복하기! 0 입력시 종료\n",
    "end = 1\n",
    "best_model = get_train_result()\n",
    "while end == 1 :\n",
    "    sentence = input(\"추출 레시피를 입력해주세요 : \")\n",
    "    if sentence == '0' :\n",
    "        break\n",
    "    predict(sentence, best_model)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f88b4f-71fe-4075-9e25-1dffc0870dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee",
   "language": "python",
   "name": "coffee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
